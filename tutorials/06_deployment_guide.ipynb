{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deployment_guide_header",
   "metadata": {},
   "source": [
    "# Model Deployment and Serving Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial provides a comprehensive guide to deploying and serving both the Mini Transformer and Advanced Transformer models in production environments. We'll cover containerization with Docker, API development with FastAPI, performance optimization, and monitoring strategies.\n",
    "\n",
    "### What You'll Learn\n",
    "- Containerizing models with Docker for consistent deployment\n",
    "- Building REST APIs with FastAPI for model serving\n",
    "- Performance optimization techniques for production\n",
    "- Load testing and benchmarking\n",
    "- Monitoring and logging strategies\n",
    "- Scaling considerations for high-traffic applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path('.').parent))\n",
    "\n",
    "# Import our model implementations\n",
    "from src.model.mini_transformer import MiniTransformer, MiniTransformerConfig\n",
    "from src.model.advanced_transformer import AdvancedTransformer, AdvancedTransformerConfig\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "docker_overview",
   "metadata": {},
   "source": [
    "## 1. Docker Containerization Overview\n",
    "\n",
    "Containerization with Docker ensures consistent deployment across different environments. Let's examine the Docker configuration files in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "docker_config_examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Docker configuration files\n",
    "docker_dir = \"../environment/docker\"\n",
    "print(f\"Docker configuration directory: {docker_dir}\")\n",
    "\n",
    "# List Docker files\n",
    "docker_files = []\n",
    "if os.path.exists(docker_dir):\n",
    "    docker_files = [f for f in os.listdir(docker_dir) if f.startswith(\"Dockerfile\")]\n",
    "    print(f\"\\nDocker files found:\")\n",
    "    for docker_file in docker_files:\n",
    "        print(f\"  - {docker_file}\")\n",
    "else:\n",
    "    print(f\"\\nDocker directory not found: {docker_dir}\")\n",
    "\n",
    "# Show content of training Dockerfile as an example\n",
    "train_dockerfile = os.path.join(docker_dir, \"Dockerfile.train\")\n",
    "if os.path.exists(train_dockerfile):\n",
    "    print(f\"\\nContent of Dockerfile.train (first 20 lines):\")\n",
    "    with open(train_dockerfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines[:20]):\n",
    "            print(f\"  {i+1:2d}: {line.rstrip()}\")\n",
    "        if len(lines) > 20:\n",
    "            print(f\"  ... ({len(lines) - 20} more lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api_development",
   "metadata": {},
   "source": [
    "## 2. FastAPI Application Development\n",
    "\n",
    "Let's create a FastAPI application for serving our models. This will provide a REST API interface for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fastapi_app_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple FastAPI application for model serving\n",
    "fastapi_code = '''\n",
    "\"\"\"\n",
    "FastAPI Application for Transformer Model Serving\n",
    "===============================================\n",
    "\n",
    "This module provides a REST API for serving transformer models.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "from src.model.mini_transformer import MiniTransformer, MiniTransformerConfig\n",
    "from src.model.advanced_transformer import AdvancedTransformer, AdvancedTransformerConfig\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Transformer Model API\",\n",
    "    description=\"API for serving Mini Transformer and Advanced Transformer models\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global model variables\n",
    "mini_model = None\n",
    "advanced_model = None\n",
    "device = None\n",
    "\n",
    "# Request/Response Models\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "    max_length: Optional[int] = 50\n",
    "    temperature: Optional[float] = 1.0\n",
    "\n",
    "class TextOutput(BaseModel):\n",
    "    generated_text: str\n",
    "    inference_time_ms: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "class ModelInfo(BaseModel):\n",
    "    model_name: str\n",
    "    parameters: int\n",
    "    device: str\n",
    "    status: str\n",
    "\n",
    "# Simple tokenizer for demonstration\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        \n",
    "        # Simple vocabulary mapping\n",
    "        self.vocab = {\n",
    "            \\'<PAD>\\': self.pad_token_id,\n",
    "            \\'<BOS>\\': self.bos_token_id,\n",
    "            \\'<EOS>\\': self.eos_token_id,\n",
    "        }\n",
    "        \n",
    "        # Add some sample words\n",
    "        words = [\n",
    "            \\'the\\', \\'of\\', \\'and\\', \\'a\\', \\'to\\', \\'in\\', \\'is\\', \\'you\\', \\'that\\', \\'it\\',\n",
    "            \\'he\\', \\'was\\', \\'for\\', \\'on\\', \\'are\\', \\'as\\', \\'with\\', \\'his\\', \\'they\\', \\'i\\',\n",
    "            \\'at\\', \\'be\\', \\'this\\', \\'have\\', \\'from\\', \\'or\\', \\'one\\', \\'had\\', \\'by\\', \\'word\\',\n",
    "            \\'but\\', \\'not\\', \\'what\\', \\'all\\', \\'were\\', \\'we\\', \\'when\\', \\'your\\', \\'can\\', \\'said\\'\n",
    "        ]\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            if i + 3 < self.vocab_size:\n",
    "                self.vocab[word] = i + 3\n",
    "        \n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
    "        tokens = [self.bos_token_id]\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.strip(\\'.,!?;:\\')\n",
    "            if word in self.vocab:\n",
    "                tokens.append(self.vocab[word])\n",
    "            else:\n",
    "                tokens.append(3)  # Unknown token\n",
    "        \n",
    "        tokens.append(self.eos_token_id)\n",
    "        \n",
    "        if max_length and len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        elif max_length and len(tokens) < max_length:\n",
    "            tokens.extend([self.pad_token_id] * (max_length - len(tokens)))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        words = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id == self.eos_token_id:\n",
    "                break\n",
    "            if token_id != self.bos_token_id and token_id != self.pad_token_id:\n",
    "                word = self.id_to_token.get(token_id, \\'<UNK>\\')\n",
    "                words.append(word)\n",
    "        return \\' \\'.join(words)\n",
    "\n",
    "# Initialize models\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models():\n",
    "    \"\"\"Load models on startup\"\"\"\n",
    "    global mini_model, advanced_model, device, tokenizer\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    \n",
    "    # Load Mini Transformer\n",
    "    try:\n",
    "        mini_config = MiniTransformerConfig(\n",
    "            vocab_size=1000,\n",
    "            hidden_size=128,\n",
    "            num_attention_heads=4,\n",
    "            num_hidden_layers=4,\n",
    "            intermediate_size=256,\n",
    "            max_position_embeddings=64,\n",
    "            dropout_prob=0.0,\n",
    "            use_cuda=torch.cuda.is_available(),\n",
    "            use_cudnn=True\n",
    "        )\n",
    "        mini_model = MiniTransformer(mini_config)\n",
    "        mini_model.to(device)\n",
    "        mini_model.eval()\n",
    "        print(\"Mini Transformer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load Mini Transformer: {e}\")\n",
    "        mini_model = None\n",
    "    \n",
    "    # Load Advanced Transformer\n",
    "    try:\n",
    "        advanced_config = AdvancedTransformerConfig(\n",
    "            hidden_size=512,\n",
    "            num_attention_heads=8,\n",
    "            num_hidden_layers=6,\n",
    "            intermediate_size=2048,\n",
    "            max_position_embeddings=512,\n",
    "            num_modalities=4,\n",
    "            gpu_acceleration_units=16,\n",
    "            spiking_neurons=False,  # Simplify for demo\n",
    "            continuous_learning=False,\n",
    "            use_cuda=torch.cuda.is_available(),\n",
    "            use_cudnn=True\n",
    "        )\n",
    "        advanced_model = AdvancedTransformer(advanced_config)\n",
    "        advanced_model.to(device)\n",
    "        advanced_model.eval()\n",
    "        print(\"Advanced Transformer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load Advanced Transformer: {e}\")\n",
    "        advanced_model = None\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\", response_model=ModelInfo)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    if mini_model is not None:\n",
    "        params = sum(p.numel() for p in mini_model.parameters())\n",
    "        return ModelInfo(\n",
    "            model_name=\"MiniTransformer\",\n",
    "            parameters=params,\n",
    "            device=str(device),\n",
    "            status=\"loaded\"\n",
    "        )\n",
    "    else:\n",
    "        return ModelInfo(\n",
    "            model_name=\"MiniTransformer\",\n",
    "            parameters=0,\n",
    "            device=str(device),\n",
    "            status=\"not loaded\"\n",
    "        )\n",
    "\n",
    "# Model information endpoint\n",
    "@app.get(\"/models\", response_model=List[ModelInfo])\n",
    "async def list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    if mini_model is not None:\n",
    "        mini_params = sum(p.numel() for p in mini_model.parameters())\n",
    "        models.append(ModelInfo(\n",
    "            model_name=\"MiniTransformer\",\n",
    "            parameters=mini_params,\n",
    "            device=str(device),\n",
    "            status=\"loaded\"\n",
    "        ))\n",
    "    \n",
    "    if advanced_model is not None:\n",
    "        advanced_params = sum(p.numel() for p in advanced_model.parameters())\n",
    "        models.append(ModelInfo(\n",
    "            model_name=\"AdvancedTransformer\",\n",
    "            parameters=advanced_params,\n",
    "            device=str(device),\n",
    "            status=\"loaded\"\n",
    "        ))\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Text generation endpoint for Mini Transformer\n",
    "@app.post(\"/generate/mini\", response_model=TextOutput)\n",
    "async def generate_text_mini(input: TextInput):\n",
    "    \"\"\"Generate text using Mini Transformer\"\"\"\n",
    "    if mini_model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Mini Transformer not loaded\")\n",
    "    \n",
    "    try:\n",
    "        # Encode input text\n",
    "        input_tokens = tokenizer.encode(input.text, max_length=32)\n",
    "        input_ids = torch.tensor([input_tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate text\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = mini_model.generate(\n",
    "                input_ids,\n",
    "                max_length=input.max_length,\n",
    "                temperature=input.temperature\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(generated_ids[0].cpu().tolist())\n",
    "        \n",
    "        return TextOutput(\n",
    "            generated_text=generated_text,\n",
    "            inference_time_ms=(end_time - start_time) * 1000,\n",
    "            input_tokens=len(input_tokens),\n",
    "            output_tokens=generated_ids.shape[1]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "# Text generation endpoint for Advanced Transformer\n",
    "@app.post(\"/generate/advanced\", response_model=TextOutput)\n",
    "async def generate_text_advanced(input: TextInput):\n",
    "    \"\"\"Generate text using Advanced Transformer\"\"\"\n",
    "    if advanced_model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Advanced Transformer not loaded\")\n",
    "    \n",
    "    try:\n",
    "        # Encode input text\n",
    "        input_tokens = tokenizer.encode(input.text, max_length=32)\n",
    "        input_ids = torch.tensor([input_tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate text\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = advanced_model.generate(\n",
    "                input_ids,\n",
    "                max_length=input.max_length,\n",
    "                temperature=input.temperature\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(generated_ids[0].cpu().tolist())\n",
    "        \n",
    "        return TextOutput(\n",
    "            generated_text=generated_text,\n",
    "            inference_time_ms=(end_time - start_time) * 1000,\n",
    "            input_tokens=len(input_tokens),\n",
    "            output_tokens=generated_ids.shape[1]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save the FastAPI application\n",
    "api_dir = \"../src/api\"\n",
    "os.makedirs(api_dir, exist_ok=True)\n",
    "api_file = os.path.join(api_dir, \"main.py\")\n",
    "\n",
    "with open(api_file, 'w') as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(f\"FastAPI application created at: {api_file}\")\n",
    "print(f\"\\nTo run the API server:\")\n",
    "print(f\"  cd src/api\")\n",
    "print(f\"  uvicorn main:app --reload\")\n",
    "print(f\"\\nAPI will be available at: http://localhost:8000\")\n",
    "print(f\"API documentation: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dockerfile_creation",
   "metadata": {},
   "source": [
    "## 3. Creating a Serving Dockerfile\n",
    "\n",
    "Let's create a Dockerfile specifically for serving our models via the FastAPI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serving_dockerfile_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dockerfile for model serving\n",
    "serving_dockerfile_content = '''\n",
    "# Serving Dockerfile with CUDA support\n",
    "FROM nvcr.io/nvidia/pytorch:23.08-py3\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV NVIDIA_VISIBLE_DEVICES=all\n",
    "ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "\n",
    "# Install additional system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    git \\\n",
    "    wget \\\n",
    "    ca-certificates \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Upgrade pip\n",
    "RUN pip install --no-cache-dir --upgrade pip\n",
    "\n",
    "# Copy requirements file\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies with CUDA optimizations\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Install additional tools for serving\n",
    "RUN pip install --no-cache-dir \\\n",
    "    fastapi \\\n",
    "    uvicorn \\\n",
    "    pydantic \\\n",
    "    python-multipart\n",
    "\n",
    "# Create directories for models and data\n",
    "RUN mkdir -p models data logs\n",
    "\n",
    "# Copy project files\n",
    "COPY . .\n",
    "\n",
    "# Set proper permissions\n",
    "RUN chmod -R 755 /app\n",
    "\n",
    "# Create non-root user with proper permissions\n",
    "RUN useradd --create-home --shell /bin/bash --uid 1000 server && \\\n",
    "    chown -R server:server /app && \\\n",
    "    usermod -aG sudo server\n",
    "\n",
    "# Switch to non-root user\n",
    "USER server\n",
    "\n",
    "# Expose port for API\n",
    "EXPOSE 8000\n",
    "\n",
    "# Set entrypoint for serving\n",
    "ENTRYPOINT [\"uvicorn\", \"src.api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# Save the serving Dockerfile\n",
    "serving_dockerfile = os.path.join(docker_dir, \"Dockerfile.serve\")\n",
    "\n",
    "with open(serving_dockerfile, 'w') as f:\n",
    "    f.write(serving_dockerfile_content)\n",
    "\n",
    "print(f\"Serving Dockerfile created at: {serving_dockerfile}\")\n",
    "print(f\"\\nTo build the serving image:\")\n",
    "print(f\"  docker build -f environment/docker/Dockerfile.serve -t transformer-serve .\")\n",
    "print(f\"\\nTo run the serving container:\")\n",
    "print(f\"  docker run -p 8000:8000 --gpus all transformer-serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kubernetes_deployment",
   "metadata": {},
   "source": [
    "## 4. Kubernetes Deployment Configuration\n",
    "\n",
    "For production deployments, Kubernetes provides orchestration capabilities. Let's create Kubernetes deployment configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kubernetes_config_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kubernetes deployment configuration\n",
    "k8s_dir = \"../environment/k8s\"\n",
    "os.makedirs(k8s_dir, exist_ok=True)\n",
    "\n",
    "# Deployment YAML\n",
    "deployment_yaml = '''\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: transformer-api\n",
    "  labels:\n",
    "    app: transformer-api\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: transformer-api\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: transformer-api\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: transformer-api\n",
    "        image: transformer-serve:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1\"\n",
    "            nvidia.com/gpu: 1\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2\"\n",
    "            nvidia.com/gpu: 1\n",
    "        env:\n",
    "        - name: NVIDIA_VISIBLE_DEVICES\n",
    "          value: \"all\"\n",
    "        - name: NVIDIA_DRIVER_CAPABILITIES\n",
    "          value: \"compute,utility\"\n",
    "      tolerations:\n",
    "      - key: \"nvidia.com/gpu\"\n",
    "        operator: \"Exists\"\n",
    "        effect: \"NoSchedule\"\n",
    "'''\n",
    "\n",
    "# Service YAML\n",
    "service_yaml = '''\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: transformer-api-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: transformer-api\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "'''\n",
    "\n",
    "# Save Kubernetes configurations\n",
    "deployment_file = os.path.join(k8s_dir, \"deployment.yaml\")\n",
    "service_file = os.path.join(k8s_dir, \"service.yaml\")\n",
    "\n",
    "with open(deployment_file, 'w') as f:\n",
    "    f.write(deployment_yaml)\n",
    "\n",
    "with open(service_file, 'w') as f:\n",
    "    f.write(service_yaml)\n",
    "\n",
    "print(f\"Kubernetes configurations created:\")\n",
    "print(f\"  Deployment: {deployment_file}\")\n",
    "print(f\"  Service: {service_file}\")\n",
    "print(f\"\\nTo deploy to Kubernetes:\")\n",
    "print(f\"  kubectl apply -f environment/k8s/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_optimization",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization Techniques\n",
    "\n",
    "Let's explore various performance optimization techniques for model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_optimization_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance Optimization Techniques for Model Serving:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Model Quantization\n",
    "print(\"\\n1. Model Quantization:\")\n",
    "print(\"   - Convert FP32 models to INT8 for reduced memory usage\")\n",
    "print(\"   - Use PyTorch's quantization tools:\")\n",
    "print(\"     import torch.quantization\")\n",
    "print(\"     model_quantized = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\")\n",
    "\n",
    "# 2. Model Compilation\n",
    "print(\"\\n2. Model Compilation:\")\n",
    "print(\"   - Use TorchScript for optimization:\")\n",
    "print(\"     scripted_model = torch.jit.script(model)\")\n",
    "print(\"   - Use ONNX for cross-platform deployment\")\n",
    "\n",
    "# 3. Batching\n",
    "print(\"\\n3. Batching Strategies:\")\n",
    "print(\"   - Process multiple requests together\")\n",
    "print(\"   - Dynamic batching based on request arrival\")\n",
    "print(\"   - Static batching for predictable workloads\")\n",
    "\n",
    "# 4. Caching\n",
    "print(\"\\n4. Caching Mechanisms:\")\n",
    "print(\"   - Cache frequent requests and responses\")\n",
    "print(\"   - Use Redis or similar for distributed caching\")\n",
    "print(\"   - Implement LRU cache for recent results\")\n",
    "\n",
    "# 5. Asynchronous Processing\n",
    "print(\"\\n5. Asynchronous Processing:\")\n",
    "print(\"   - Use async/await in FastAPI\")\n",
    "print(\"   - Implement request queues for heavy processing\")\n",
    "print(\"   - Use background tasks for non-critical operations\")\n",
    "\n",
    "# 6. Load Balancing\n",
    "print(\"\\n6. Load Balancing:\")\n",
    "print(\"   - Use NGINX or similar for request distribution\")\n",
    "print(\"   - Implement health checks for automatic failover\")\n",
    "print(\"   - Use Kubernetes services for built-in load balancing\")\n",
    "\n",
    "# 7. Monitoring and Metrics\n",
    "print(\"\\n7. Monitoring and Metrics:\")\n",
    "print(\"   - Track inference latency and throughput\")\n",
    "print(\"   - Monitor GPU and memory utilization\")\n",
    "print(\"   - Use Prometheus and Grafana for visualization\")\n",
    "print(\"   - Implement logging for debugging and audit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_testing",
   "metadata": {},
   "source": [
    "## 6. Load Testing and Benchmarking\n",
    "\n",
    "Let's create a simple load testing script to benchmark our API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_testing_script",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a load testing script\n",
    "load_test_code = '''\n",
    "\"\"\"\n",
    "Load Testing Script for Transformer API\n",
    "=====================================\n",
    "\n",
    "This script performs load testing on the transformer API.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import statistics\n",
    "from typing import List\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "async def send_request(session, prompt: str) -> float:\n",
    "    \"\"\"Send a single request and return response time\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        async with session.post(\n",
    "            f\"{API_URL}/generate/mini\",\n",
    "            json={\n",
    "                \"text\": prompt,\n",
    "                \"max_length\": 30,\n",
    "                \"temperature\": 0.8\n",
    "            }\n",
    "        ) as response:\n",
    "            await response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return time.time() - start_time\n",
    "\n",
    "async def load_test(concurrent_requests: int = 10, total_requests: int = 100):\n",
    "    \"\"\"Perform load testing with specified concurrency\"\"\"\n",
    "    # Sample prompts for testing\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"Machine learning is transforming\",\n",
    "        \"Natural language processing enables\",\n",
    "        \"Deep learning models can\",\n",
    "        \"Ethical AI development requires\"\n",
    "    ]\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create tasks for all requests\n",
    "        tasks = []\n",
    "        for i in range(total_requests):\n",
    "            prompt = prompts[i % len(prompts)]\n",
    "            tasks.append(send_request(session, prompt))\n",
    "        \n",
    "        # Execute requests\n",
    "        start_time = time.time()\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Filter out failed requests\n",
    "        response_times = [r for r in results if isinstance(r, float)]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if response_times:\n",
    "            avg_response_time = statistics.mean(response_times)\n",
    "            median_response_time = statistics.median(response_times)\n",
    "            min_response_time = min(response_times)\n",
    "            max_response_time = max(response_times)\n",
    "            \n",
    "            print(f\"Load Test Results:\")\n",
    "            print(f\"  Total requests: {total_requests}\")\n",
    "            print(f\"  Concurrent requests: {concurrent_requests}\")\n",
    "            print(f\"  Successful requests: {len(response_times)}\")\n",
    "            print(f\"  Failed requests: {total_requests - len(response_times)}\")\n",
    "            print(f\"  Total test time: {total_time:.2f} seconds\")\n",
    "            print(f\"  Requests per second: {total_requests / total_time:.2f}\")\n",
    "            print(f\"  Average response time: {avg_response_time*1000:.2f} ms\")\n",
    "            print(f\"  Median response time: {median_response_time*1000:.2f} ms\")\n",
    "            print(f\"  Min response time: {min_response_time*1000:.2f} ms\")\n",
    "            print(f\"  Max response time: {max_response_time*1000:.2f} ms\")\n",
    "        else:\n",
    "            print(\"All requests failed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting load test...\")\n",
    "    asyncio.run(load_test(concurrent_requests=5, total_requests=20))\n",
    "'''\n",
    "\n",
    "# Save the load testing script\n",
    "test_dir = \"../tests\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "load_test_file = os.path.join(test_dir, \"load_test.py\")\n",
    "\n",
    "with open(load_test_file, 'w') as f:\n",
    "    f.write(load_test_code)\n",
    "\n",
    "print(f\"Load testing script created at: {load_test_file}\")\n",
    "print(f\"\\nTo run the load test (after starting the API server):\")\n",
    "print(f\"  python tests/load_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring_logging",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Logging\n",
    "\n",
    "Let's enhance our FastAPI application with monitoring and logging capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitoring_logging_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced FastAPI code with monitoring and logging\n",
    "enhanced_fastapi_code = '''\n",
    "\"\"\"\n",
    "Enhanced FastAPI Application with Monitoring and Logging\n",
    "=======================================================\n",
    "\n",
    "This module provides a REST API for serving transformer models with enhanced monitoring.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import asyncio\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "from src.model.mini_transformer import MiniTransformer, MiniTransformerConfig\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Transformer Model API\",\n",
    "    description=\"API for serving transformer models with monitoring\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global variables\n",
    "mini_model = None\n",
    "device = None\n",
    "request_count = 0\n",
    "total_inference_time = 0.0\n",
    "\n",
    "# Request/Response Models\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "    max_length: Optional[int] = 50\n",
    "    temperature: Optional[float] = 1.0\n",
    "\n",
    "class TextOutput(BaseModel):\n",
    "    generated_text: str\n",
    "    inference_time_ms: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "class HealthInfo(BaseModel):\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    device: str\n",
    "    request_count: int\n",
    "    average_response_time_ms: Optional[float]\n",
    "    cpu_percent: float\n",
    "    memory_percent: float\n",
    "\n",
    "# Simple tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        \n",
    "        self.vocab = {\\'<PAD>\\': self.pad_token_id, \\'<BOS>\\': self.bos_token_id, \\'<EOS>\\': self.eos_token_id}\n",
    "        words = [\\'the\\', \\'of\\', \\'and\\', \\'a\\', \\'to\\', \\'in\\', \\'is\\', \\'you\\', \\'that\\', \\'it\\']\n",
    "        for i, word in enumerate(words):\n",
    "            if i + 3 < self.vocab_size:\n",
    "                self.vocab[word] = i + 3\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
    "        tokens = [self.bos_token_id] + [self.vocab.get(w, 3) for w in text.lower().split()[:max_length-2]] + [self.eos_token_id]\n",
    "        if max_length and len(tokens) < max_length:\n",
    "            tokens.extend([self.pad_token_id] * (max_length - len(tokens)))\n",
    "        return tokens[:max_length] if max_length else tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        return \\' \\'.join([self.id_to_token.get(t, \\'<UNK>\\') for t in token_ids if t not in [self.bos_token_id, self.eos_token_id, self.pad_token_id]])\n",
    "\n",
    "# Middleware for logging requests\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.time() - start_time\n",
    "    logger.info(f\"{request.method} {request.url.path} - Status: {response.status_code} - Time: {process_time:.4f}s\")\n",
    "    return response\n",
    "\n",
    "# Initialize models\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models():\n",
    "    \"\"\"Load models on startup\"\"\"\n",
    "    global mini_model, device, tokenizer\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = SimpleTokenizer()\n",
    "    \n",
    "    try:\n",
    "        config = MiniTransformerConfig(vocab_size=1000, hidden_size=128, num_attention_heads=4, num_hidden_layers=4)\n",
    "        mini_model = MiniTransformer(config)\n",
    "        mini_model.to(device)\n",
    "        mini_model.eval()\n",
    "        logger.info(\"Mini Transformer loaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load Mini Transformer: {e}\")\n",
    "        mini_model = None\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\", response_model=HealthInfo)\n",
    "async def health_check():\n",
    "    \"\"\"Enhanced health check with system metrics\"\"\"\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    \n",
    "    avg_response_time = total_inference_time / request_count if request_count > 0 else None\n",
    "    \n",
    "    return HealthInfo(\n",
    "        status=\"healthy\" if mini_model is not None else \"unhealthy\",\n",
    "        model_loaded=mini_model is not None,\n",
    "        device=str(device),\n",
    "        request_count=request_count,\n",
    "        average_response_time_ms=avg_response_time,\n",
    "        cpu_percent=cpu_percent,\n",
    "        memory_percent=memory_percent\n",
    "    )\n",
    "\n",
    "# Text generation endpoint\n",
    "@app.post(\"/generate\", response_model=TextOutput)\n",
    "async def generate_text(input: TextInput):\n",
    "    \"\"\"Generate text with monitoring\"\"\"\n",
    "    global request_count, total_inference_time\n",
    "    \n",
    "    if mini_model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
    "    \n",
    "    request_count += 1\n",
    "    logger.info(f\"Processing request #{request_count}: {input.text[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        input_tokens = tokenizer.encode(input.text, max_length=32)\n",
    "        input_ids = torch.tensor([input_tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = mini_model.generate(input_ids, max_length=input.max_length, temperature=input.temperature)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        total_inference_time += inference_time\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated_ids[0].cpu().tolist())\n",
    "        \n",
    "        logger.info(f\"Request #{request_count} completed in {inference_time*1000:.2f}ms\")\n",
    "        \n",
    "        return TextOutput(\n",
    "            generated_text=generated_text,\n",
    "            inference_time_ms=inference_time * 1000,\n",
    "            input_tokens=len(input_tokens),\n",
    "            output_tokens=generated_ids.shape[1]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Generation failed for request #{request_count}: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "print(\"Enhanced FastAPI application with monitoring and logging:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Key enhancements include:\")\n",
    "print(\"  1. Comprehensive logging with timestamps and request details\")\n",
    "print(\"  2. System metrics monitoring (CPU, memory usage)\")\n",
    "print(\"  3. Request counting and performance tracking\")\n",
    "print(\"  4. Middleware for request logging\")\n",
    "print(\"  5. Enhanced health check endpoint with metrics\")\n",
    "print(\"  6. Error handling with detailed logging\")\n",
    "print(\"\\nTo implement this enhanced version, replace the content of src/api/main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling_considerations",
   "metadata": {},
   "source": [
    "## 8. Scaling Considerations\n",
    "\n",
    "Let's discuss important scaling considerations for production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling_considerations_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaling Considerations for Production Deployments:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Horizontal Scaling\n",
    "print(\"\\n1. Horizontal Scaling:\")\n",
    "print(\"   - Use load balancers to distribute requests\")\n",
    "print(\"   - Implement stateless services for easy scaling\")\n",
    "print(\"   - Use Kubernetes for automatic scaling based on metrics\")\n",
    "print(\"   - Consider blue-green deployments for zero-downtime updates\")\n",
    "\n",
    "# Vertical Scaling\n",
    "print(\"\\n2. Vertical Scaling:\")\n",
    "print(\"   - Use larger GPU instances for better performance\")\n",
    "print(\"   - Optimize memory usage to fit larger models\")\n",
    "print(\"   - Consider model parallelism for very large models\")\n",
    "print(\"   - Use mixed precision training/inference to reduce memory\")\n",
    "\n",
    "# Auto-scaling\n",
    "print(\"\\n3. Auto-scaling Strategies:\")\n",
    "print(\"   - Scale based on CPU/GPU utilization\")\n",
    "print(\"   - Scale based on request queue length\")\n",
    "print(\"   - Scale based on response time SLAs\")\n",
    "print(\"   - Implement predictive scaling based on traffic patterns\")\n",
    "\n",
    "# Geographic Distribution\n",
    "print(\"\\n4. Geographic Distribution:\")\n",
    "print(\"   - Deploy to multiple regions for low latency\")\n",
    "print(\"   - Use CDN for static content\")\n",
    "print(\"   - Implement data replication for consistency\")\n",
    "print(\"   - Consider edge computing for real-time applications\")\n",
    "\n",
    "# Cost Optimization\n",
    "print(\"\\n5. Cost Optimization:\")\n",
    "print(\"   - Use spot instances for non-critical workloads\")\n",
    "print(\"   - Implement request batching to improve utilization\")\n",
    "print(\"   - Use model compression techniques\")\n",
    "print(\"   - Monitor and optimize resource allocation\")\n",
    "\n",
    "# Fault Tolerance\n",
    "print(\"\\n6. Fault Tolerance:\")\n",
    "print(\"   - Implement circuit breakers for downstream services\")\n",
    "print(\"   - Use health checks for automatic failover\")\n",
    "print(\"   - Implement retry logic with exponential backoff\")\n",
    "print(\"   - Use distributed tracing for debugging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has covered the essential aspects of deploying and serving transformer models in production environments:\n",
    "\n",
    "1. **Containerization with Docker**: Ensuring consistent deployment across environments\n",
    "2. **API Development with FastAPI**: Creating RESTful interfaces for model serving\n",
    "3. **Kubernetes Deployment**: Orchestrating containers for scalability\n",
    "4. **Performance Optimization**: Techniques for maximizing throughput and minimizing latency\n",
    "5. **Load Testing**: Validating system performance under various loads\n",
    "6. **Monitoring and Logging**: Tracking system health and performance metrics\n",
    "7. **Scaling Considerations**: Strategies for handling increased traffic and demand\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Start Simple**: Begin with basic containerization and API development\n",
    "- **Monitor Everything**: Implement comprehensive logging and monitoring from the start\n",
    "- **Plan for Growth**: Design systems with scalability in mind\n",
    "- **Optimize Continuously**: Regularly benchmark and optimize performance\n",
    "- **Ensure Reliability**: Implement fault tolerance and graceful degradation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. Use infrastructure as code (Kubernetes YAML, Dockerfiles) for reproducible deployments\n",
    "2. Implement comprehensive monitoring and alerting\n",
    "3. Design stateless services for easy scaling\n",
    "4. Use load testing to validate performance under expected loads\n",
    "5. Implement proper error handling and logging\n",
    "6. Plan for disaster recovery and backup strategies\n",
    "7. Regularly update and patch container images\n",
    "8. Use secrets management for sensitive configuration\n",
    "\n",
    "These deployment strategies and techniques form the foundation of robust, scalable, and maintainable AI model serving infrastructure used by leading technology companies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}