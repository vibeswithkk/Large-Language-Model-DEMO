{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7778488c",
   "metadata": {},
   "source": [
    "# Mini Transformer Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial provides a comprehensive walkthrough of the Mini Transformer implementation, a simplified version of the Transformer architecture designed for educational purposes. The Mini Transformer demonstrates the core concepts of attention mechanisms and self-attention without the complexity of full-scale models.\n",
    "\n",
    "### What You'll Learn\n",
    "- The fundamental components of a Transformer model\n",
    "- How multi-head attention works\n",
    "- Implementation of Transformer layers\n",
    "- Model configuration and initialization\n",
    "- Forward pass through the network\n",
    "- Practical usage and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8703ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path('.').parent))\n",
    "\n",
    "# Import our Mini Transformer implementation\n",
    "from src.model.mini_transformer import MiniTransformer, MiniTransformerConfig\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef829f",
   "metadata": {},
   "source": [
    "## 1. Understanding the Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in the paper \"Attention Is All You Need\" by Vaswani et al., revolutionized natural language processing by replacing recurrent layers with self-attention mechanisms. The key components include:\n",
    "\n",
    "1. **Multi-Head Attention**: Allows the model to focus on different parts of the input simultaneously\n",
    "2. **Feed-Forward Networks**: Position-wise fully connected layers\n",
    "3. **Layer Normalization**: Stabilizes training\n",
    "4. **Residual Connections**: Enable training of deep networks\n",
    "\n",
    "Our Mini Transformer simplifies these concepts while maintaining the core functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82c4b5",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "The `MiniTransformerConfig` class defines all the hyperparameters for our model. Let's examine and create a configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model configuration\n",
    "config = MiniTransformerConfig(\n",
    "    vocab_size=1000,          # Size of vocabulary\n",
    "    hidden_size=128,          # Dimension of hidden layers\n",
    "    num_attention_heads=4,    # Number of attention heads\n",
    "    num_hidden_layers=4,      # Number of Transformer layers\n",
    "    intermediate_size=256,    # Size of feed-forward layers\n",
    "    max_position_embeddings=64, # Maximum sequence length\n",
    "    dropout_prob=0.1,         # Dropout probability\n",
    "    use_cuda=torch.cuda.is_available(), # Enable CUDA optimizations\n",
    "    use_cudnn=True            # Enable cuDNN optimizations\n",
    ")\n",
    "\n",
    "print(\"Mini Transformer Configuration:\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  Hidden layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Intermediate size: {config.intermediate_size}\")\n",
    "print(f\"  Max position embeddings: {config.max_position_embeddings}\")\n",
    "print(f\"  Dropout probability: {config.dropout_prob}\")\n",
    "print(f\"  Head dimension: {config.head_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa29dc",
   "metadata": {},
   "source": [
    "## 3. Creating the Model\n",
    "\n",
    "Now let's create an instance of our Mini Transformer using the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = MiniTransformer(config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Show model structure\n",
    "print(f\"\\nModel structure:\")\n",
    "print(f\"  Embedding layers: token_embeddings, position_embeddings\")\n",
    "print(f\"  Transformer layers: {len(model.layers)} layers\")\n",
    "print(f\"  Final layer norm and LM head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371da6e",
   "metadata": {},
   "source": [
    "## 4. Understanding Model Components\n",
    "\n",
    "Let's examine the individual components of our Mini Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80523701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine embedding layers\n",
    "print(\"Embedding Layers:\")\n",
    "print(f\"  Token embeddings shape: {model.token_embeddings.weight.shape}\")\n",
    "print(f\"  Position embeddings shape: {model.position_embeddings.weight.shape}\")\n",
    "\n",
    "# Examine first Transformer layer\n",
    "first_layer = model.layers[0]\n",
    "print(f\"\\nFirst Transformer Layer:\")\n",
    "print(f\"  Attention mechanism: {type(first_layer.attention).__name__}\")\n",
    "print(f\"  Layer norm 1: {type(first_layer.layer_norm1).__name__}\")\n",
    "print(f\"  Feed-forward network: {type(first_layer.ffn).__name__}\")\n",
    "print(f\"  Layer norm 2: {type(first_layer.layer_norm2).__name__}\")\n",
    "\n",
    "# Examine attention mechanism in detail\n",
    "attention = first_layer.attention\n",
    "print(f\"\\nAttention Mechanism:\")\n",
    "print(f\"  Hidden size: {attention.hidden_size}\")\n",
    "print(f\"  Number of heads: {attention.num_heads}\")\n",
    "print(f\"  Head dimension: {attention.head_dim}\")\n",
    "print(f\"  Q projection: {attention.q_proj}\")\n",
    "print(f\"  K projection: {attention.k_proj}\")\n",
    "print(f\"  V projection: {attention.v_proj}\")\n",
    "print(f\"  Output projection: {attention.o_proj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d551e45",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Through the Model\n",
    "\n",
    "Let's perform a forward pass through our Mini Transformer with sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58334d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input data\n",
    "batch_size = 4\n",
    "sequence_length = 16\n",
    "\n",
    "# Generate random token IDs\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, sequence_length)).to(device)\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Sample input IDs: {input_ids[0][:10]}...\")\n",
    "\n",
    "# Perform forward pass\n",
    "start_time = time.time()\n",
    "outputs = model(input_ids)\n",
    "forward_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nForward pass completed in {forward_time:.4f} seconds\")\n",
    "print(f\"Logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"Hidden states shape: {outputs['hidden_states'].shape}\")\n",
    "\n",
    "# Examine logits\n",
    "logits = outputs['logits']\n",
    "print(f\"\\nLogits statistics:\")\n",
    "print(f\"  Mean: {logits.mean().item():.4f}\")\n",
    "print(f\"  Std: {logits.std().item():.4f}\")\n",
    "print(f\"  Min: {logits.min().item():.4f}\")\n",
    "print(f\"  Max: {logits.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93b00f",
   "metadata": {},
   "source": [
    "## 6. Training with Loss Computation\n",
    "\n",
    "Let's see how the model computes loss for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for loss computation (shifted input for language modeling)\n",
    "labels = torch.randint(0, config.vocab_size, (batch_size, sequence_length)).to(device)\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Forward pass with labels to compute loss\n",
    "start_time = time.time()\n",
    "outputs_with_loss = model(input_ids, labels=labels)\n",
    "loss_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nForward pass with loss computation in {loss_time:.4f} seconds\")\n",
    "print(f\"Logits shape: {outputs_with_loss['logits'].shape}\")\n",
    "print(f\"Loss: {outputs_with_loss['loss'].item():.4f}\")\n",
    "\n",
    "# Examine loss computation details\n",
    "logits_flat = outputs_with_loss['logits'].view(-1, config.vocab_size)\n",
    "labels_flat = labels.view(-1)\n",
    "print(f\"\\nFlattened logits shape: {logits_flat.shape}\")\n",
    "print(f\"Flattened labels shape: {labels_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d621e9",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of our Mini Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b887af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, input_ids, iterations=100):\n",
    "    \"\"\"Benchmark model performance\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(input_ids)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = model(input_ids)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / iterations\n",
    "    sequences_per_second = (input_ids.shape[0] * input_ids.shape[1]) / avg_time\n",
    "    \n",
    "    return avg_time, sequences_per_second\n",
    "\n",
    "# Benchmark the model\n",
    "avg_time, seq_per_sec = benchmark_model(model, input_ids)\n",
    "\n",
    "print(f\"Performance Analysis:\")\n",
    "print(f\"  Average forward pass time: {avg_time*1000:.2f} ms\")\n",
    "print(f\"  Sequences per second: {seq_per_sec:.2f}\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Parameters per sequence element: {total_params/(batch_size*sequence_length):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58d513",
   "metadata": {},
   "source": [
    "## 8. Gradient Computation and Backward Pass\n",
    "\n",
    "Let's examine how gradients are computed during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab815c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient computation\n",
    "model.train()\n",
    "\n",
    "# Forward pass with loss\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss = outputs['loss']\n",
    "\n",
    "print(f\"Loss before backward pass: {loss.item():.4f}\")\n",
    "print(f\"Loss requires gradient: {loss.requires_grad}\")\n",
    "\n",
    "# Check gradients before backward pass\n",
    "first_layer_weights = model.layers[0].attention.q_proj.weight\n",
    "print(f\"\\nFirst layer Q-projection weights require gradient: {first_layer_weights.requires_grad}\")\n",
    "print(f\"First layer Q-projection gradients present: {first_layer_weights.grad is not None}\")\n",
    "\n",
    "# Backward pass\n",
    "start_time = time.time()\n",
    "loss.backward()\n",
    "backward_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBackward pass completed in {backward_time:.4f} seconds\")\n",
    "print(f\"First layer Q-projection gradients present: {first_layer_weights.grad is not None}\")\n",
    "print(f\"First layer Q-projection gradient norm: {first_layer_weights.grad.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fff558",
   "metadata": {},
   "source": [
    "## 9. Model Optimization Features\n",
    "\n",
    "Our Mini Transformer implementation includes several optimization features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c063112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine CUDA optimizations\n",
    "print(\"CUDA Optimizations:\")\n",
    "print(f\"  Model uses CUDA: {model.use_cuda}\")\n",
    "print(f\"  cuDNN benchmarking enabled: {torch.backends.cudnn.benchmark}\")\n",
    "\n",
    "# Examine weight initialization\n",
    "print(f\"\\nWeight Initialization:\")\n",
    "embedding_weights = model.token_embeddings.weight\n",
    "print(f\"  Embedding weights mean: {embedding_weights.mean().item():.4f}\")\n",
    "print(f\"  Embedding weights std: {embedding_weights.std().item():.4f}\")\n",
    "\n",
    "attention_weights = model.layers[0].attention.q_proj.weight\n",
    "print(f\"  Attention weights mean: {attention_weights.mean().item():.4f}\")\n",
    "print(f\"  Attention weights std: {attention_weights.std().item():.4f}\")\n",
    "\n",
    "# Examine layer normalization\n",
    "layer_norm = model.layers[0].layer_norm1\n",
    "print(f\"\\nLayer Normalization:\")\n",
    "print(f\"  Weight mean: {layer_norm.weight.mean().item():.4f}\")\n",
    "print(f\"  Weight std: {layer_norm.weight.std().item():.4f}\")\n",
    "print(f\"  Bias mean: {layer_norm.bias.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb424ac2",
   "metadata": {},
   "source": [
    "## 10. Practical Usage Examples\n",
    "\n",
    "Let's look at some practical examples of how to use the Mini Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randint(0, config.vocab_size, (1, 8)).to(device)\n",
    "    outputs = model(sample_input)\n",
    "    \n",
    "    print(\"Example 1: Simple Inference\")\n",
    "    print(f\"  Input shape: {sample_input.shape}\")\n",
    "    print(f\"  Output logits shape: {outputs['logits'].shape}\")\n",
    "    print(f\"  Hidden states shape: {outputs['hidden_states'].shape}\")\n",
    "\n",
    "# Example 2: Getting predictions\n",
    "with torch.no_grad():\n",
    "    logits = outputs['logits']\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"\\nExample 2: Predictions\")\n",
    "    print(f\"  Predicted token IDs: {predictions[0].tolist()}\")\n",
    "    print(f\"  Prediction probabilities shape: {torch.softmax(logits, dim=-1).shape}\")\n",
    "\n",
    "# Example 3: Model saving and loading\n",
    "print(f\"\\nExample 3: Model Persistence\")\n",
    "save_path = \"mini_transformer_checkpoint.pth\"\n",
    "\n",
    "# Save model state\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"  Model saved to {save_path}\")\n",
    "\n",
    "# Create new model and load weights\n",
    "new_model = MiniTransformer(config).to(device)\n",
    "new_model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "print(f\"  Model loaded successfully\")\n",
    "\n",
    "# Verify the models produce the same output\n",
    "with torch.no_grad():\n",
    "    original_output = model(sample_input)\n",
    "    loaded_output = new_model(sample_input)\n",
    "    \n",
    "    outputs_match = torch.allclose(original_output['logits'], loaded_output['logits'], atol=1e-6)\n",
    "    print(f\"  Outputs match: {outputs_match}\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "if os.path.exists(save_path):\n",
    "    os.remove(save_path)\n",
    "    print(f\"  Cleanup: Removed {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f4353",
   "metadata": {},
   "source": [
    "## 11. Scaling Considerations\n",
    "\n",
    "Let's examine how the model scales with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scaling():\n",
    "    \"\"\"Analyze how model parameters scale with configuration\"\"\"\n",
    "    configs = [\n",
    "        {\"name\": \"Small\", \"hidden_size\": 64, \"num_heads\": 2, \"num_layers\": 2, \"intermediate_size\": 128},\n",
    "        {\"name\": \"Medium\", \"hidden_size\": 128, \"num_heads\": 4, \"num_layers\": 4, \"intermediate_size\": 256},\n",
    "        {\"name\": \"Large\", \"hidden_size\": 256, \"num_heads\": 8, \"num_layers\": 6, \"intermediate_size\": 512}\n",
    "    ]\n",
    "    \n",
    "    print(\"Scaling Analysis:\")\n",
    "    print(f\"{'Config':<10} {'Hidden':<8} {'Heads':<6} {'Layers':<7} {'FFN':<8} {'Params':<12} {'Ratio':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    small_params = None\n",
    "    for config_spec in configs:\n",
    "        config = MiniTransformerConfig(\n",
    "            vocab_size=1000,\n",
    "            hidden_size=config_spec[\"hidden_size\"],\n",
    "            num_attention_heads=config_spec[\"num_heads\"],\n",
    "            num_hidden_layers=config_spec[\"num_layers\"],\n",
    "            intermediate_size=config_spec[\"intermediate_size\"],\n",
    "            max_position_embeddings=64\n",
    "        )\n",
    "        \n",
    "        model = MiniTransformer(config)\n",
    "        params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        if small_params is None:\n",
    "            small_params = params\n",
    "        \n",
    "        ratio = params / small_params\n",
    "        \n",
    "        print(f\"{config_spec['name']:<10} {config_spec['hidden_size']:<8} {config_spec['num_heads']:<6} {config_spec['num_layers']:<7} {config_spec['intermediate_size']:<8} {params:<12,} {ratio:<8.1f}\")\n",
    "\n",
    "analyze_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec45dc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've explored the Mini Transformer implementation in depth:\n",
    "\n",
    "- **Architecture**: Understanding the core components of the Transformer model\n",
    "- **Configuration**: How to set up model hyperparameters\n",
    "- **Implementation**: Examining the code structure and components\n",
    "- **Forward Pass**: How data flows through the network\n",
    "- **Training**: Loss computation and gradient calculation\n",
    "- **Optimizations**: CUDA and cuDNN optimizations for performance\n",
    "- **Practical Usage**: Real-world examples of model usage\n",
    "- **Scaling**: How model complexity grows with configuration\n",
    "\n",
    "The Mini Transformer serves as an excellent educational tool that demonstrates the fundamental concepts of attention mechanisms and Transformer architectures while remaining accessible for learning and experimentation. This implementation forms the foundation for understanding more complex models like the Advanced Transformer in this repository."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
