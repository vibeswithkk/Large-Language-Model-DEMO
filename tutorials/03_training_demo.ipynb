{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fe6137",
   "metadata": {},
   "source": [
    "# Training Demo Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial provides a comprehensive demonstration of training workflows for both the Mini Transformer and Advanced Transformer models. We'll explore different training approaches, from simple toy-scale training to enterprise-scale distributed training with DeepSpeed.\n",
    "\n",
    "### What You'll Learn\n",
    "- Setting up training environments\n",
    "- Configuring training parameters\n",
    "- Implementing training loops\n",
    "- Monitoring training progress\n",
    "- Optimizing training performance\n",
    "- Best practices for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f125ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path('.').parent))\n",
    "\n",
    "# Import our model implementations\n",
    "from src.model.mini_transformer import MiniTransformer, MiniTransformerConfig\n",
    "from src.training.train_toy import TextDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba1fc0",
   "metadata": {},
   "source": [
    "## 1. Training Environment Setup\n",
    "\n",
    "Before training, we need to set up the appropriate environment and configure CUDA optimizations for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998672cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_environment():\n",
    "    \"\"\"Setup training environment with CUDA optimizations\"\"\"\n",
    "    # Enable cuDNN benchmarking for better performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TensorFloat-32 for better performance on modern GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"Training environment setup completed\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "    print(f\"cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"TensorFloat-32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Setup environment\n",
    "device = setup_training_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8cde9",
   "metadata": {},
   "source": [
    "## 2. Creating Sample Data\n",
    "\n",
    "For demonstration purposes, we'll create sample training data that simulates text data for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe91421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_texts(num_samples=1000):\n",
    "    \"\"\"Create sample text data for training\"\"\"\n",
    "    base_texts = [\n",
    "        \"The field of artificial intelligence has seen tremendous growth in recent years.\",\n",
    "        \"Machine learning algorithms can learn patterns from data without explicit programming.\",\n",
    "        \"Deep learning models, particularly neural networks, have achieved remarkable results.\",\n",
    "        \"Natural language processing enables computers to understand and generate human language.\",\n",
    "        \"Computer vision allows machines to interpret and understand visual information.\",\n",
    "        \"Reinforcement learning trains agents to make decisions through trial and error.\",\n",
    "        \"Data science combines statistics, programming, and domain expertise to extract insights.\",\n",
    "        \"Big data technologies handle the storage and processing of massive datasets.\",\n",
    "        \"Cloud computing provides scalable resources for machine learning workloads.\",\n",
    "        \"Ethical AI ensures that artificial intelligence systems are fair and unbiased.\"\n",
    "    ]\n",
    "    \n",
    "    # Generate a larger dataset\n",
    "    sample_texts = []\n",
    "    for i in range(num_samples):\n",
    "        text = base_texts[i % len(base_texts)] + \" \" + base_texts[(i + 1) % len(base_texts)]\n",
    "        sample_texts.append(text[:128])  # Limit text length\n",
    "    \n",
    "    return sample_texts\n",
    "\n",
    "# Create sample data\n",
    "sample_texts = create_sample_texts(5000)\n",
    "print(f\"Created {len(sample_texts)} sample texts\")\n",
    "print(f\"Sample text: {sample_texts[0][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc1ea",
   "metadata": {},
   "source": [
    "## 3. Dataset and DataLoader Configuration\n",
    "\n",
    "We'll create a dataset and configure a DataLoader for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487261a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = TextDataset(sample_texts, max_length=64)\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "# Create DataLoader with optimizations\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,  # Enable pinned memory for faster GPU transfer\n",
    "    num_workers=2,    # Use multiple workers for data loading\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=2  # Prefetch data for better performance\n",
    ")\n",
    "\n",
    "print(f\"DataLoader configured with batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Examine a sample batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  Labels shape: {sample_batch['labels'].shape}\")\n",
    "print(f\"  Device: {sample_batch['input_ids'].device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65aa68",
   "metadata": {},
   "source": [
    "## 4. Model Configuration and Initialization\n",
    "\n",
    "Let's configure and initialize our Mini Transformer model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62334c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "config = MiniTransformerConfig(\n",
    "    vocab_size=10000,\n",
    "    hidden_size=256,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=64,\n",
    "    dropout_prob=0.1,\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    use_cudnn=True\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = MiniTransformer(config)\n",
    "model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  Hidden layers: {config.num_hidden_layers}\")\n",
    "print(f\"  \\nModel created successfully\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b877615",
   "metadata": {},
   "source": [
    "## 5. Optimizer and Learning Rate Scheduler\n",
    "\n",
    "We'll set up an optimizer and learning rate scheduler for effective training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a9b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer with weight decay for better generalization\n",
    "learning_rate = 5e-4\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Create learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=1000,  # Adjust based on your needs\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"Optimizer configured:\")\n",
    "print(f\"  Type: {type(optimizer).__name__}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Weight decay: 0.01\")\n",
    "print(f\"\\nLearning rate scheduler:\")\n",
    "print(f\"  Type: {type(scheduler).__name__}\")\n",
    "print(f\"  T_max: 1000\")\n",
    "print(f\"  Eta min: 1e-6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef7882",
   "metadata": {},
   "source": [
    "## 6. Training Loop Implementation\n",
    "\n",
    "Let's implement a training loop with proper monitoring and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d46137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, scheduler, device, num_epochs=3):\n",
    "    \"\"\"Train the model with monitoring and logging\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize metrics tracking\n",
    "    epoch_losses = []\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress tracking\n",
    "        batch_times = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Move tensors to device with non-blocking transfer\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad(set_to_none=True)  # More memory efficient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            batch_time = time.time() - batch_start_time\n",
    "            batch_times.append(batch_time)\n",
    "            \n",
    "            # Log progress periodically\n",
    "            if batch_idx % 50 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                avg_batch_time = np.mean(batch_times[-50:]) if len(batch_times) >= 50 else np.mean(batch_times)\n",
    "                samples_per_sec = input_ids.size(0) / batch_time\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx:3d}, \"\n",
    "                      f\"Loss: {loss.item():.4f}, LR: {current_lr:.6f}, \"\n",
    "                      f\"Batch Time: {batch_time:.3f}s, Samples/sec: {samples_per_sec:.1f}\")\n",
    "        \n",
    "        # End of epoch metrics\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        epoch_losses.append(avg_loss)\n",
    "        epoch_times.append(epoch_time)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} completed:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Epoch Time: {epoch_time:.2f}s\")\n",
    "        print(f\"  Average Batch Time: {np.mean(batch_times):.3f}s\")\n",
    "        print(f\"  Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return epoch_losses, epoch_times\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_start_time = time.time()\n",
    "epoch_losses, epoch_times = train_model(model, dataloader, optimizer, scheduler, device, num_epochs=2)\n",
    "total_train_time = time.time() - train_start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {total_train_time:.2f}s\")\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9109bb",
   "metadata": {},
   "source": [
    "## 7. Training Metrics Visualization\n",
    "\n",
    "Let's visualize the training metrics to understand the model's learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss over epochs\n",
    "ax1.plot(range(1, len(epoch_losses) + 1), epoch_losses, 'b-o')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Over Epochs')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot epoch times\n",
    "ax2.bar(range(1, len(epoch_times) + 1), epoch_times, color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Epoch Training Times')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {epoch_losses[-1]:.4f}\")\n",
    "print(f\"Loss improvement: {epoch_losses[0] - epoch_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef7643",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model with some basic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluate the model on validation data\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            total_loss += outputs[\"loss\"].item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "# Evaluate the model\n",
    "eval_loss, perplexity = evaluate_model(model, dataloader, device)\n",
    "\n",
    "print(f\"Model Evaluation:\")\n",
    "print(f\"  Validation Loss: {eval_loss:.4f}\")\n",
    "print(f\"  Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aee2e7",
   "metadata": {},
   "source": [
    "## 9. Model Saving and Loading\n",
    "\n",
    "Let's save our trained model and demonstrate how to load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c98d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"training_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = output_dir / \"mini_transformer_trained.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / \"config.json\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config.__dict__, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "# Demonstrate loading\n",
    "print(f\"\\nLoading model...\")\n",
    "loaded_model = MiniTransformer(config)\n",
    "loaded_model.to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(f\"Model loaded successfully\")\n",
    "\n",
    "# Verify the models produce the same output\n",
    "model.eval()\n",
    "loaded_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(dataloader))\n",
    "    input_ids = sample_batch[\"input_ids\"].to(device)\n",
    "    \n",
    "    original_output = model(input_ids, labels=sample_batch[\"labels\"].to(device))\n",
    "    loaded_output = loaded_model(input_ids, labels=sample_batch[\"labels\"].to(device))\n",
    "    \n",
    "    outputs_match = torch.allclose(original_output[\"loss\"], loaded_output[\"loss\"], atol=1e-6)\n",
    "    print(f\"Outputs match: {outputs_match}\")\n",
    "\n",
    "# Clean up\n",
    "if model_path.exists():\n",
    "    os.remove(model_path)\n",
    "if config_path.exists():\n",
    "    os.remove(config_path)\n",
    "if output_dir.exists() and not any(output_dir.iterdir()):\n",
    "    output_dir.rmdir()\n",
    "    \n",
    "print(f\"\\nCleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88374c",
   "metadata": {},
   "source": [
    "## 10. Performance Optimization Techniques\n",
    "\n",
    "Let's explore some advanced performance optimization techniques for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_training_optimizations():\n",
    "    \"\"\"Benchmark different training optimization techniques\"\"\"\n",
    "    print(\"Performance Optimization Benchmarking:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a small dataset for benchmarking\n",
    "    small_texts = create_sample_texts(100)\n",
    "    small_dataset = TextDataset(small_texts, max_length=32)\n",
    "    \n",
    "    optimizations = [\n",
    "        {\"name\": \"Baseline\", \"pin_memory\": False, \"num_workers\": 0},\n",
    "        {\"name\": \"Pinned Memory\", \"pin_memory\": True, \"num_workers\": 0},\n",
    "        {\"name\": \"Multi-worker\", \"pin_memory\": False, \"num_workers\": 2},\n",
    "        {\"name\": \"Optimized\", \"pin_memory\": True, \"num_workers\": 2},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for opt in optimizations:\n",
    "        # Create DataLoader with specific optimization\n",
    "        dataloader = DataLoader(\n",
    "            small_dataset,\n",
    "            batch_size=8,\n",
    "            shuffle=True,\n",
    "            pin_memory=opt[\"pin_memory\"],\n",
    "            num_workers=opt[\"num_workers\"],\n",
    "            persistent_workers=opt[\"num_workers\"] > 0\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        config = MiniTransformerConfig(\n",
    "            vocab_size=10000,\n",
    "            hidden_size=128,\n",
    "            num_attention_heads=2,\n",
    "            num_hidden_layers=2,\n",
    "            intermediate_size=256,\n",
    "            max_position_embeddings=32\n",
    "        )\n",
    "        model = MiniTransformer(config).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(2):\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device, non_blocking=opt[\"pin_memory\"])\n",
    "                labels = batch[\"labels\"].to(device, non_blocking=opt[\"pin_memory\"])\n",
    "                outputs = model(input_ids, labels=labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        for _ in range(5):\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device, non_blocking=opt[\"pin_memory\"])\n",
    "                labels = batch[\"labels\"].to(device, non_blocking=opt[\"pin_memory\"])\n",
    "                outputs = model(input_ids, labels=labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs[\"loss\"].backward()\n",
    "                optimizer.step()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 5\n",
    "        results.append((opt[\"name\"], avg_time))\n",
    "        \n",
    "        print(f\"{opt['name']:<15}: {avg_time:.3f}s per epoch\")\n",
    "    \n",
    "    # Show improvement\n",
    "    baseline_time = results[0][1]\n",
    "    best_time = min(results, key=lambda x: x[1])[1]\n",
    "    improvement = (baseline_time - best_time) / baseline_time * 100\n",
    "    \n",
    "    print(f\"\\nPerformance improvement: {improvement:.1f}% with optimized settings\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    benchmark_training_optimizations()\n",
    "else:\n",
    "    print(\"Performance benchmarking requires CUDA availability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00c27f",
   "metadata": {},
   "source": [
    "## 11. Gradient Analysis\n",
    "\n",
    "Let's analyze the gradients during training to understand the learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32530ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model):\n",
    "    \"\"\"Analyze gradient statistics\"\"\"\n",
    "    total_norm = 0\n",
    "    layer_norms = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            layer_norms.append((name, param_norm.item()))\n",
    "    \n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    \n",
    "    print(f\"Gradient Analysis:\")\n",
    "    print(f\"  Total gradient norm: {total_norm:.4f}\")\n",
    "    print(f\"  \\nTop 5 layers by gradient norm:\")\n",
    "    layer_norms.sort(key=lambda x: x[1], reverse=True)\n",
    "    for name, norm in layer_norms[:5]:\n",
    "        print(f\"    {name}: {norm:.4f}\")\n",
    "    \n",
    "    return total_norm, layer_norms\n",
    "\n",
    "# Analyze gradients (requires a backward pass first)\n",
    "sample_batch = next(iter(dataloader))\n",
    "input_ids = sample_batch[\"input_ids\"].to(device)\n",
    "labels = sample_batch[\"labels\"].to(device)\n",
    "\n",
    "outputs = model(input_ids, labels=labels)\n",
    "outputs[\"loss\"].backward()\n",
    "\n",
    "total_grad_norm, layer_grad_norms = analyze_gradients(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abd800",
   "metadata": {},
   "source": [
    "## 12. Memory Usage Monitoring\n",
    "\n",
    "Let's monitor memory usage during training to understand resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory_usage():\n",
    "    \"\"\"Monitor GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "        \n",
    "        print(f\"Memory Usage Monitoring:\")\n",
    "        print(f\"  Allocated memory: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved memory: {reserved:.2f} GB\")\n",
    "        print(f\"  Max allocated memory: {max_allocated:.2f} GB\")\n",
    "        \n",
    "        return allocated, reserved, max_allocated\n",
    "    else:\n",
    "        print(\"Memory monitoring requires CUDA availability\")\n",
    "        return None, None, None\n",
    "\n",
    "memory_stats = monitor_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127bdf3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've demonstrated a comprehensive training workflow for the Mini Transformer:\n",
    "\n",
    "- **Environment Setup**: Configuring CUDA optimizations for better performance\n",
    "- **Data Preparation**: Creating datasets and efficient data loaders\n",
    "- **Model Configuration**: Setting up model hyperparameters\n",
    "- **Optimizer Setup**: Configuring AdamW optimizer with learning rate scheduling\n",
    "- **Training Loop**: Implementing a robust training loop with monitoring\n",
    "- **Performance Metrics**: Tracking loss, timing, and other metrics\n",
    "- **Model Persistence**: Saving and loading trained models\n",
    "- **Optimization Techniques**: Benchmarking different performance optimizations\n",
    "- **Gradient Analysis**: Understanding learning dynamics through gradient monitoring\n",
    "- **Memory Monitoring**: Tracking resource usage during training\n",
    "\n",
    "This training demo provides a solid foundation for understanding how to train Transformer models effectively. The techniques demonstrated here can be scaled up for training larger models and adapted for different architectures. For enterprise-scale training, consider using the DeepSpeed training script which provides additional optimizations for distributed training across multiple GPUs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
