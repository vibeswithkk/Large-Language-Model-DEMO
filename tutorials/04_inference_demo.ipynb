{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf38ded7",
   "metadata": {},
   "source": [
    "# Inference Demo Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial provides a comprehensive demonstration of inference workflows for both the Mini Transformer and Advanced Transformer models. We'll explore different inference techniques, from simple text generation to advanced sampling strategies.\n",
    "\n",
    "### What You'll Learn\n",
    "- Setting up inference environments\n",
    "- Loading pre-trained models\n",
    "- Implementing text generation\n",
    "- Using different sampling strategies\n",
    "- Optimizing inference performance\n",
    "- Evaluating model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path('.').parent))\n",
    "\n",
    "# Import our model implementations\n",
    "from src.model.mini_transformer import MiniTransformer, MiniTransformerConfig\n",
    "from src.inference.run_inference import TextGenerator\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472cb02",
   "metadata": {},
   "source": [
    "## 1. Inference Environment Setup\n",
    "\n",
    "Before inference, we need to set up the appropriate environment and configure CUDA optimizations for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_inference_environment():\n",
    "    \"\"\"Setup inference environment with CUDA optimizations\"\"\"\n",
    "    # Enable cuDNN benchmarking for better performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TensorFloat-32 for better performance on modern GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"Inference environment setup completed\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "    print(f\"cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"TensorFloat-32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Setup environment\n",
    "device = setup_inference_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971d20a",
   "metadata": {},
   "source": [
    "## 2. Model Configuration and Initialization\n",
    "\n",
    "Let's configure and initialize our Mini Transformer model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "config = MiniTransformerConfig(\n",
    "    vocab_size=10000,\n",
    "    hidden_size=256,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=128,\n",
    "    dropout_prob=0.1,\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    use_cudnn=True\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = MiniTransformer(config)\n",
    "model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  Hidden layers: {config.num_hidden_layers}\")\n",
    "print(f\"  \\nModel created successfully\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model in evaluation mode: {not model.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c46be",
   "metadata": {},
   "source": [
    "## 3. Text Generation Basics\n",
    "\n",
    "Let's start with basic text generation using greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, input_ids, max_length=50):\n",
    "    \"\"\"Generate text using greedy decoding\"\"\"\n",
    "    model.eval()\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(generated)\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            # Get the most likely next token\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Create sample input\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "input_tokens = [hash(c) % config.vocab_size for c in prompt]\n",
    "input_ids = torch.tensor([input_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Input prompt: {prompt}\")\n",
    "print(f\"Input tokens: {input_tokens[:10]}...\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Generate text\n",
    "start_time = time.time()\n",
    "generated_ids = greedy_decode(model, input_ids, max_length=30)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGeneration completed in {generation_time:.4f}s\")\n",
    "print(f\"Generated sequence length: {generated_ids.shape[1]}\")\n",
    "\n",
    "# Convert back to text (simplified representation)\n",
    "generated_tokens = generated_ids[0].cpu().tolist()\n",
    "generated_text = ''.join([chr(token % 128) for token in generated_tokens[len(input_tokens):]])\n",
    "print(f\"Generated text: {prompt + generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14743b90",
   "metadata": {},
   "source": [
    "## 4. Using the TextGenerator Class\n",
    "\n",
    "Let's use the TextGenerator class from our inference module for more advanced generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7278469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TextGenerator instance\n",
    "generator = TextGenerator(model, device=device)\n",
    "\n",
    "print(f\"TextGenerator created successfully\")\n",
    "print(f\"Model device: {generator.device}\")\n",
    "print(f\"Model compiled: {hasattr(torch, 'compile') and generator.model != model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e033c9d",
   "metadata": {},
   "source": [
    "## 5. Sampling Strategies\n",
    "\n",
    "Let's explore different sampling strategies for more diverse text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a31f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature sampling\n",
    "prompt = \"Machine learning is\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nTemperature Sampling:\")\n",
    "\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "for temp in temperatures:\n",
    "    start_time = time.time()\n",
    "    generated_text = generator.generate_text(\n",
    "        prompt, \n",
    "        max_length=30, \n",
    "        temperature=temp, \n",
    "        do_sample=True\n",
    "    )\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Temperature {temp}: {generated_text[len(prompt):50]}... ({generation_time:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k and Top-p sampling\n",
    "prompt = \"The advancement of\"\n",
    "\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(\"\\nTop-k and Top-p Sampling:\")\n",
    "\n",
    "# Top-k sampling\n",
    "start_time = time.time()\n",
    "generated_k = generator.generate_text(\n",
    "    prompt,\n",
    "    max_length=30,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=1.0\n",
    ")\n",
    "time_k = time.time() - start_time\n",
    "\n",
    "print(f\"  Top-k (k=50): {generated_k[len(prompt):50]}... ({time_k:.3f}s)\")\n",
    "\n",
    "# Top-p (nucleus) sampling\n",
    "start_time = time.time()\n",
    "generated_p = generator.generate_text(\n",
    "    prompt,\n",
    "    max_length=30,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    "    top_k=0,  # Disable top-k\n",
    "    top_p=0.9\n",
    ")\n",
    "time_p = time.time() - start_time\n",
    "\n",
    "print(f\"  Top-p (p=0.9): {generated_p[len(prompt):50]}... ({time_p:.3f}s)\")\n",
    "\n",
    "# Combined top-k and top-p\n",
    "start_time = time.time()\n",
    "generated_kp = generator.generate_text(\n",
    "    prompt,\n",
    "    max_length=30,\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "time_kp = time.time() - start_time\n",
    "\n",
    "print(f\"  Top-k + Top-p: {generated_kp[len(prompt):50]}... ({time_kp:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e1c9d",
   "metadata": {},
   "source": [
    "## 6. Next Token Predictions\n",
    "\n",
    "Let's explore how to get predictions for the next token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10744d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_next_token_predictions(generator, text, top_k=5):\n",
    "    \"\"\"Show next token predictions with probabilities\"\"\"\n",
    "    predictions = generator.get_next_token_predictions(text, top_k=top_k)\n",
    "    \n",
    "    print(f\"Text: \\\"{text}\\\"\")\n",
    "    print(\"Next token predictions:\")\n",
    "    \n",
    "    for i, (token, prob) in enumerate(zip(predictions[\"tokens\"], predictions[\"probabilities\"])):\n",
    "        # Convert token to character (simplified)\n",
    "        char = chr(token % 128)\n",
    "        print(f\"  {i+1:2d}. Token {token:5d} ('{char}') - Probability: {prob:.4f}\")\n",
    "\n",
    "# Show predictions\n",
    "show_next_token_predictions(generator, \"Artificial intelligence\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f02be1",
   "metadata": {},
   "source": [
    "## 7. Performance Benchmarking\n",
    "\n",
    "Let's benchmark the inference performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, generator, device, iterations=100):\n",
    "    \"\"\"Benchmark inference performance\"\"\"\n",
    "    prompt = \"The quick brown fox jumps over the lazy dog. \"\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = generator.generate_text(prompt, max_length=10, do_sample=False)\n",
    "    \n",
    "    # Benchmark greedy decoding\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = generator.generate_text(prompt, max_length=20, do_sample=False)\n",
    "    greedy_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark sampling\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = generator.generate_text(prompt, max_length=20, do_sample=True, temperature=1.0)\n",
    "    sampling_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark next token predictions\n",
    "    start_time = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = generator.get_next_token_predictions(prompt, top_k=10)\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    avg_greedy = greedy_time / iterations\n",
    "    avg_sampling = sampling_time / iterations\n",
    "    avg_prediction = prediction_time / iterations\n",
    "    \n",
    "    return avg_greedy, avg_sampling, avg_prediction\n",
    "\n",
    "# Run benchmark\n",
    "greedy_time, sampling_time, prediction_time = benchmark_inference(model, generator, device, iterations=50)\n",
    "\n",
    "print(\"Inference Performance Benchmark:\")\n",
    "print(f\"  Greedy decoding: {greedy_time*1000:.2f} ms per generation\")\n",
    "print(f\"  Sampling: {sampling_time*1000:.2f} ms per generation\")\n",
    "print(f\"  Next token prediction: {prediction_time*1000:.2f} ms per prediction\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"  Memory allocated: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1ccb8",
   "metadata": {},
   "source": [
    "## 8. Batch Inference\n",
    "\n",
    "Let's explore batch inference for processing multiple prompts simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a04f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(model, prompts, max_length=20):\n",
    "    \"\"\"Generate text for multiple prompts in batch\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize all prompts\n",
    "    batch_input_ids = []\n",
    "    prompt_lengths = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        input_tokens = [hash(c) % 10000 for c in prompt]\n",
    "        batch_input_ids.append(torch.tensor(input_tokens, dtype=torch.long))\n",
    "        prompt_lengths.append(len(input_tokens))\n",
    "    \n",
    "    # Pad to same length\n",
    "    max_prompt_length = max(prompt_lengths)\n",
    "    padded_input_ids = []\n",
    "    \n",
    "    for input_ids in batch_input_ids:\n",
    "        padding = torch.zeros(max_prompt_length - len(input_ids), dtype=torch.long)\n",
    "        padded_ids = torch.cat([input_ids, padding])\n",
    "        padded_input_ids.append(padded_ids)\n",
    "    \n",
    "    # Stack into batch\n",
    "    batch_input = torch.stack(padded_input_ids).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated = batch_input.clone()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            outputs = model(generated)\n",
    "            logits = outputs[\"logits\"]\n",
    "            \n",
    "            # Get next tokens for all sequences\n",
    "            next_tokens = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_tokens], dim=1)\n",
    "    \n",
    "    # Convert back to text\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        generated_tokens = generated[i].cpu().tolist()\n",
    "        # Remove padding and convert to text\n",
    "        actual_tokens = generated_tokens[:prompt_lengths[i] + max_length]\n",
    "        generated_text = ''.join([chr(token % 128) for token in actual_tokens[prompt_lengths[i]:]])\n",
    "        results.append(prompt + generated_text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test batch inference\n",
    "prompts = [\n",
    "    \"The future of AI\",\n",
    "    \"Machine learning\",\n",
    "    \"Deep learning models\",\n",
    "    \"Natural language processing\"\n",
    "]\n",
    "\n",
    "print(\"Batch Inference Example:\")\n",
    "print(f\"Processing {len(prompts)} prompts simultaneously\")\n",
    "\n",
    "start_time = time.time()\n",
    "batch_results = batch_generate(model, prompts, max_length=15)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nBatch processing completed in {batch_time:.4f}s\")\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"  {i+1}. \\\"{result[:50]}...\\\"\")\n",
    "\n",
    "# Compare with sequential processing\n",
    "start_time = time.time()\n",
    "sequential_results = []\n",
    "for prompt in prompts:\n",
    "    result = generator.generate_text(prompt, max_length=15, do_sample=False)\n",
    "    sequential_results.append(result)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSequential processing completed in {sequential_time:.4f}s\")\n",
    "print(f\"Speedup: {sequential_time/batch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e2d92",
   "metadata": {},
   "source": [
    "## 9. Memory Optimization Techniques\n",
    "\n",
    "Let's explore memory optimization techniques for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b412584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_memory_optimization():\n",
    "    \"\"\"Demonstrate memory optimization techniques\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Memory optimization demonstration requires CUDA availability\")\n",
    "        return\n",
    "    \n",
    "    print(\"Memory Optimization Techniques:\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    initial_memory = torch.cuda.memory_allocated()\n",
    "    \n",
    "    print(f\"  Initial memory: {initial_memory / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Create a large tensor to demonstrate memory usage\n",
    "    large_tensor = torch.randn(1000, 1000, device='cuda')\n",
    "    after_allocation = torch.cuda.memory_allocated()\n",
    "    \n",
    "    print(f\"  After large tensor: {after_allocation / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Delete tensor\n",
    "    del large_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    after_cleanup = torch.cuda.memory_allocated()\n",
    "    \n",
    "    print(f\"  After cleanup: {after_cleanup / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Demonstrate torch.no_grad context\n",
    "    print(f\"\\n  Gradient computation memory impact:\")\n",
    "    \n",
    "    # With gradients\n",
    "    torch.cuda.empty_cache()\n",
    "    before_grad = torch.cuda.memory_allocated()\n",
    "    \n",
    "    input_ids = torch.randint(0, 10000, (4, 32), device='cuda')\n",
    "    outputs = model(input_ids)\n",
    "    loss = outputs[\"logits\"].sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    with_grad_memory = torch.cuda.memory_allocated()\n",
    "    print(f\"    With gradients: {(with_grad_memory - before_grad) / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Without gradients\n",
    "    torch.cuda.empty_cache()\n",
    "    before_no_grad = torch.cuda.memory_allocated()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    without_grad_memory = torch.cuda.memory_allocated()\n",
    "    print(f\"    Without gradients: {(without_grad_memory - before_no_grad) / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "demonstrate_memory_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdca70",
   "metadata": {},
   "source": [
    "## 10. Model Compilation for Performance\n",
    "\n",
    "Let's explore model compilation for improved inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_model_compilation():\n",
    "    \"\"\"Demonstrate model compilation for performance improvement\"\"\"\n",
    "    if not hasattr(torch, 'compile'):\n",
    "        print(\"Model compilation requires PyTorch 2.0+\")\n",
    "        return\n",
    "    \n",
    "    print(\"Model Compilation:\")\n",
    "    \n",
    "    # Create a new model for compilation demo\n",
    "    config_small = MiniTransformerConfig(\n",
    "        vocab_size=1000,\n",
    "        hidden_size=128,\n",
    "        num_attention_heads=2,\n",
    "        num_hidden_layers=2,\n",
    "        intermediate_size=256,\n",
    "        max_position_embeddings=64\n",
    "    )\n",
    "    \n",
    "    model_small = MiniTransformer(config_small).to(device)\n",
    "    \n",
    "    # Create sample input\n",
    "    input_ids = torch.randint(0, 1000, (2, 16), device=device)\n",
    "    \n",
    "    # Benchmark uncompiled model\n",
    "    start_time = time.time()\n",
    "    for _ in range(20):\n",
    "        with torch.no_grad():\n",
    "            _ = model_small(input_ids)\n",
    "    uncompiled_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Uncompiled model: {uncompiled_time*1000/20:.2f} ms per forward pass\")\n",
    "    \n",
    "    # Compile model\n",
    "    try:\n",
    "        compiled_model = torch.compile(model_small)\n",
    "        \n",
    "        # Warmup compiled model\n",
    "        for _ in range(5):\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_ids)\n",
    "        \n",
    "        # Benchmark compiled model\n",
    "        start_time = time.time()\n",
    "        for _ in range(20):\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_ids)\n",
    "        compiled_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  Compiled model: {compiled_time*1000/20:.2f} ms per forward pass\")\n",
    "        print(f\"  Speedup: {uncompiled_time/compiled_time:.2f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Compilation failed: {e}\")\n",
    "\n",
    "if torch.cuda.is_available() and hasattr(torch, 'compile'):\n",
    "    demonstrate_model_compilation()\n",
    "else:\n",
    "    print(\"Model compilation demonstration requires CUDA and PyTorch 2.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc59d1b",
   "metadata": {},
   "source": [
    "## 11. Advanced Generation Techniques\n",
    "\n",
    "Let's explore some advanced text generation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, input_ids, max_length=20, num_beams=3):\n",
    "    \"\"\"Simple beam search implementation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize beams\n",
    "    beams = [(input_ids.clone(), 0.0)]  # (sequence, cumulative_log_prob)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            candidates = []\n",
    "            \n",
    "            for sequence, score in beams:\n",
    "                outputs = model(sequence)\n",
    "                logits = outputs[\"logits\"]\n",
    "                \n",
    "                # Get top-k logits for this beam\n",
    "                log_probs = F.log_softmax(logits[:, -1, :], dim=-1)\n",
    "                top_log_probs, top_indices = torch.topk(log_probs, num_beams, dim=-1)\n",
    "                \n",
    "                # Add candidates\n",
    "                for i in range(num_beams):\n",
    "                    new_sequence = torch.cat([sequence, top_indices[:, i:i+1]], dim=1)\n",
    "                    new_score = score + top_log_probs[:, i].item()\n",
    "                    candidates.append((new_sequence, new_score))\n",
    "            \n",
    "            # Select top beams\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            beams = candidates[:num_beams]\n",
    "    \n",
    "    # Return the best sequence\n",
    "    return beams[0][0]\n",
    "\n",
    "# Test beam search\n",
    "prompt = \"The impact of\"\n",
    "input_tokens = [hash(c) % config.vocab_size for c in prompt]\n",
    "input_ids = torch.tensor([input_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Advanced Generation Techniques:\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "# Greedy decoding\n",
    "start_time = time.time()\n",
    "greedy_result = generator.generate_text(prompt, max_length=20, do_sample=False)\n",
    "greedy_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGreedy decoding: \\\"{greedy_result[len(prompt):50]}...\\\" ({greedy_time:.3f}s)\")\n",
    "\n",
    "# Beam search\n",
    "start_time = time.time()\n",
    "beam_result_ids = beam_search_decode(model, input_ids, max_length=20, num_beams=3)\n",
    "beam_time = time.time() - start_time\n",
    "\n",
    "# Convert beam result to text\n",
    "beam_tokens = beam_result_ids[0].cpu().tolist()\n",
    "beam_text = ''.join([chr(token % 128) for token in beam_tokens[len(input_tokens):]])\n",
    "print(f\"Beam search (k=3): \\\"{prompt + beam_text[:50]}...\\\" ({beam_time:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e1bd4",
   "metadata": {},
   "source": [
    "## 12. Model Analysis\n",
    "\n",
    "Let's analyze some properties of our model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cea37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_outputs(model, input_ids):\n",
    "    \"\"\"Analyze model outputs during inference\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs[\"logits\"]\n",
    "        hidden_states = outputs[\"hidden_states\"]\n",
    "    \n",
    "    print(f\"Model Output Analysis:\")\n",
    "    print(f\"  Logits shape: {logits.shape}\")\n",
    "    print(f\"  Hidden states shape: {hidden_states.shape}\")\n",
    "    \n",
    "    # Analyze logits\n",
    "    print(f\"\\nLogits Analysis:\")\n",
    "    print(f\"  Mean: {logits.mean().item():.4f}\")\n",
    "    print(f\"  Std: {logits.std().item():.4f}\")\n",
    "    print(f\"  Min: {logits.min().item():.4f}\")\n",
    "    print(f\"  Max: {logits.max().item():.4f}\")\n",
    "    \n",
    "    # Analyze entropy (measure of uncertainty)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    entropy = -(probs * torch.log(probs + 1e-12)).sum(dim=-1)\n",
    "    \n",
    "    print(f\"\\nUncertainty Analysis:\")\n",
    "    print(f\"  Average entropy: {entropy.mean().item():.4f}\")\n",
    "    print(f\"  Min entropy: {entropy.min().item():.4f}\")\n",
    "    print(f\"  Max entropy: {entropy.max().item():.4f}\")\n",
    "    \n",
    "    # Analyze hidden states\n",
    "    print(f\"\\nHidden States Analysis:\")\n",
    "    print(f\"  Mean activation: {hidden_states.mean().item():.4f}\")\n",
    "    print(f\"  Std activation: {hidden_states.std().item():.4f}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Analyze model outputs\n",
    "sample_input = torch.randint(0, config.vocab_size, (1, 16), device=device)\n",
    "outputs = analyze_model_outputs(model, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36978d9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've demonstrated comprehensive inference workflows for the Mini Transformer:\n",
    "\n",
    "- **Environment Setup**: Configuring CUDA optimizations for better performance\n",
    "- **Model Configuration**: Setting up model hyperparameters for inference\n",
    "- **Basic Generation**: Implementing greedy decoding for deterministic output\n",
    "- **Sampling Strategies**: Using temperature, top-k, and top-p sampling for diverse outputs\n",
    "- **Batch Inference**: Processing multiple prompts simultaneously for efficiency\n",
    "- **Performance Benchmarking**: Measuring inference speed and optimization impact\n",
    "- **Memory Optimization**: Techniques for efficient memory usage\n",
    "- **Model Compilation**: Using PyTorch 2.0+ compilation for performance gains\n",
    "- **Advanced Techniques**: Beam search and other sophisticated generation methods\n",
    "- **Model Analysis**: Understanding model behavior during inference\n",
    "\n",
    "This inference demo provides a solid foundation for understanding how to deploy Transformer models for text generation tasks. The techniques demonstrated here can be scaled up for larger models and adapted for different architectures. For production deployment, consider using the FastAPI serving script which provides additional optimizations for serving models in production environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
