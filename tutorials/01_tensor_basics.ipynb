{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92d4095",
   "metadata": {},
   "source": [
    "# Tensor Basics Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial provides a comprehensive introduction to tensors, the fundamental data structure used in deep learning frameworks like PyTorch. Understanding tensors is crucial for working with neural networks and large language models.\n",
    "\n",
    "### What You'll Learn\n",
    "- What tensors are and how they differ from arrays\n",
    "- Creating and manipulating tensors in PyTorch\n",
    "- Tensor operations and broadcasting\n",
    "- Memory management and performance considerations\n",
    "- Practical applications in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65968b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449be887",
   "metadata": {},
   "source": [
    "## 1. What Are Tensors?\n",
    "\n",
    "Tensors are multi-dimensional arrays that generalize scalars, vectors, and matrices to higher dimensions. In the context of deep learning:\n",
    "\n",
    "- **0D Tensor**: Scalar (single number)\n",
    "- **1D Tensor**: Vector (list of numbers)\n",
    "- **2D Tensor**: Matrix (table of numbers)\n",
    "- **3D+ Tensor**: Higher-dimensional arrays\n",
    "\n",
    "Tensors in PyTorch are similar to NumPy arrays but with additional capabilities for GPU acceleration and automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors of different dimensions\n",
    "\n",
    "# 0D Tensor (Scalar)\n",
    "scalar = torch.tensor(5.0)\n",
    "print(f\"0D Tensor (Scalar): {scalar}\")\n",
    "print(f\"Shape: {scalar.shape}, Dimensions: {scalar.dim()}\")\n",
    "\n",
    "# 1D Tensor (Vector)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"\\n1D Tensor (Vector): {vector}\")\n",
    "print(f\"Shape: {vector.shape}, Dimensions: {vector.dim()}\")\n",
    "\n",
    "# 2D Tensor (Matrix)\n",
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(f\"\\n2D Tensor (Matrix):\\n{matrix}\")\n",
    "print(f\"Shape: {matrix.shape}, Dimensions: {matrix.dim()}\")\n",
    "\n",
    "# 3D Tensor\n",
    "tensor_3d = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n",
    "print(f\"\\n3D Tensor:\\n{tensor_3d}\")\n",
    "print(f\"Shape: {tensor_3d.shape}, Dimensions: {tensor_3d.dim()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643a3ea",
   "metadata": {},
   "source": [
    "## 2. Creating Tensors\n",
    "\n",
    "PyTorch provides several ways to create tensors with different properties and initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors with different initialization methods\n",
    "\n",
    "# From Python lists\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "\n",
    "# Zeros tensor\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "print(f\"\\nZeros tensor:\\n{zeros_tensor}\")\n",
    "\n",
    "# Ones tensor\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "print(f\"\\nOnes tensor:\\n{ones_tensor}\")\n",
    "\n",
    "# Random tensor\n",
    "random_tensor = torch.rand(2, 3)\n",
    "print(f\"\\nRandom tensor:\\n{random_tensor}\")\n",
    "\n",
    "# Normal distribution tensor\n",
    "normal_tensor = torch.randn(2, 3)\n",
    "print(f\"\\nNormal distribution tensor:\\n{normal_tensor}\")\n",
    "\n",
    "# Range tensor\n",
    "range_tensor = torch.arange(0, 10, 2)\n",
    "print(f\"\\nRange tensor: {range_tensor}\")\n",
    "\n",
    "# Linearly spaced tensor\n",
    "linspace_tensor = torch.linspace(0, 1, 5)\n",
    "print(f\"\\nLinearly spaced tensor: {linspace_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeea622",
   "metadata": {},
   "source": [
    "## 3. Tensor Properties and Data Types\n",
    "\n",
    "Tensors have various properties that define their behavior, including data types, devices, and memory layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining tensor properties\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "print(f\"Tensor:\\n{tensor}\")\n",
    "print(f\"Data type: {tensor.dtype}\")\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print(f\"Device: {tensor.device}\")\n",
    "print(f\"Number of elements: {tensor.numel()}\")\n",
    "print(f\"Memory layout: {tensor.layout}\")\n",
    "\n",
    "# Changing data types\n",
    "int_tensor = tensor.to(torch.int64)\n",
    "print(f\"\\nInteger tensor:\\n{int_tensor}\")\n",
    "print(f\"Data type: {int_tensor.dtype}\")\n",
    "\n",
    "# Moving to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = tensor.cuda()\n",
    "    print(f\"\\nGPU tensor device: {gpu_tensor.device}\")\n",
    "else:\n",
    "    print(\"\\nCUDA not available, skipping GPU example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc63f6",
   "metadata": {},
   "source": [
    "## 4. Tensor Operations\n",
    "\n",
    "PyTorch provides a rich set of operations for manipulating tensors, from basic arithmetic to advanced linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic arithmetic operations\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"\\nAddition (a + b): {a + b}\")\n",
    "print(f\"Subtraction (a - b): {a - b}\")\n",
    "print(f\"Multiplication (a * b): {a * b}\")\n",
    "print(f\"Division (a / b): {a / b}\")\n",
    "\n",
    "# Using PyTorch functions\n",
    "print(f\"\\nUsing torch.add(): {torch.add(a, b)}\")\n",
    "print(f\"Using torch.mul(): {torch.mul(a, b)}\")\n",
    "\n",
    "# In-place operations (modify tensor directly)\n",
    "c = a.clone()\n",
    "c.add_(b)  # In-place addition\n",
    "print(f\"\\nIn-place addition result: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "\n",
    "matrix_a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "matrix_b = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(f\"Matrix A:\\n{matrix_a}\")\n",
    "print(f\"Matrix B:\\n{matrix_b}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "matmul_result = torch.matmul(matrix_a, matrix_b)\n",
    "print(f\"\\nMatrix multiplication (A @ B):\\n{matmul_result}\")\n",
    "\n",
    "# Element-wise multiplication\n",
    "elementwise_result = matrix_a * matrix_b\n",
    "print(f\"\\nElement-wise multiplication:\\n{elementwise_result}\")\n",
    "\n",
    "# Transpose\n",
    "transposed = matrix_a.T\n",
    "print(f\"\\nTransposed matrix A:\\n{transposed}\")\n",
    "\n",
    "# Determinant\n",
    "det = torch.det(matrix_a)\n",
    "print(f\"\\nDeterminant of A: {det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea1931",
   "metadata": {},
   "source": [
    "## 5. Broadcasting\n",
    "\n",
    "Broadcasting allows operations between tensors of different shapes under certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1552981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting examples\n",
    "\n",
    "# Adding a scalar to a matrix\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "scalar = 10.0\n",
    "\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "print(f\"Scalar: {scalar}\")\n",
    "print(f\"\\nMatrix + scalar:\\n{matrix + scalar}\")\n",
    "\n",
    "# Adding a vector to a matrix\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"\\nVector: {vector}\")\n",
    "print(f\"\\nMatrix + vector:\\n{matrix + vector}\")\n",
    "\n",
    "# Broadcasting rules demonstration\n",
    "a = torch.tensor([1.0, 2.0, 3.0])  # Shape: [3]\n",
    "b = torch.tensor([[1.0], [2.0], [3.0]])  # Shape: [3, 1]\n",
    "\n",
    "print(f\"\\nVector a shape: {a.shape}\")\n",
    "print(f\"Vector b shape: {b.shape}\")\n",
    "print(f\"\\nBroadcasted addition result shape: {(a + b).shape}\")\n",
    "print(f\"Broadcasted addition result:\\n{a + b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da23590",
   "metadata": {},
   "source": [
    "## 6. Indexing and Slicing\n",
    "\n",
    "Accessing and modifying specific elements or subsets of tensors is a common operation in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and slicing examples\n",
    "\n",
    "tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(f\"Original tensor:\\n{tensor}\")\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "\n",
    "# Accessing elements\n",
    "print(f\"\\nFirst element [0,0,0]: {tensor[0, 0, 0]}\")\n",
    "print(f\"Element at [1,0,2]: {tensor[1, 0, 2]}\")\n",
    "\n",
    "# Slicing\n",
    "print(f\"\\nFirst 'page':\\n{tensor[0]}\")\n",
    "print(f\"\\nFirst row of each page:\\n{tensor[:, 0]}\")\n",
    "print(f\"\\nFirst column of first page:\\n{tensor[0, :, 0]}\")\n",
    "\n",
    "# Advanced indexing\n",
    "indices = torch.tensor([0, 1])\n",
    "print(f\"\\nUsing indices {indices}:\\n{tensor[indices]}\")\n",
    "\n",
    "# Boolean indexing\n",
    "mask = tensor > 5\n",
    "print(f\"\\nBoolean mask:\\n{mask}\")\n",
    "print(f\"Elements > 5: {tensor[mask]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa9d10",
   "metadata": {},
   "source": [
    "## 7. Reshaping and View Operations\n",
    "\n",
    "Reshaping tensors is essential for preparing data for neural networks and manipulating tensor dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605bc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and view operations\n",
    "\n",
    "tensor = torch.arange(12)\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "\n",
    "# Reshape to 3x4 matrix\n",
    "reshaped = tensor.reshape(3, 4)\n",
    "print(f\"\\nReshaped to 3x4:\\n{reshaped}\")\n",
    "\n",
    "# Reshape to 2x2x3 tensor\n",
    "reshaped_3d = tensor.reshape(2, 2, 3)\n",
    "print(f\"\\nReshaped to 2x2x3:\\n{reshaped_3d}\")\n",
    "\n",
    "# Using view (creates a view of the same data)\n",
    "viewed = tensor.view(4, 3)\n",
    "print(f\"\\nViewed as 4x3:\\n{viewed}\")\n",
    "\n",
    "# Flatten\n",
    "flattened = reshaped_3d.flatten()\n",
    "print(f\"\\nFlattened: {flattened}\")\n",
    "\n",
    "# Squeeze and unsqueeze\n",
    "single_dim = torch.tensor([[[1, 2, 3]]])\n",
    "print(f\"\\nSingle dim tensor shape: {single_dim.shape}\")\n",
    "squeezed = single_dim.squeeze()\n",
    "print(f\"Squeezed shape: {squeezed.shape}\")\n",
    "unsqueezed = squeezed.unsqueeze(0).unsqueeze(0)\n",
    "print(f\"Unsqueezed shape: {unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd485a",
   "metadata": {},
   "source": [
    "## 8. Performance Considerations\n",
    "\n",
    "Understanding memory management and performance optimization is crucial for working with large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac487420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: In-place vs out-of-place operations\n",
    "\n",
    "def time_operation(func, *args, iterations=1000):\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result = func(*args)\n",
    "    end = time.time()\n",
    "    return (end - start) / iterations\n",
    "\n",
    "# Create large tensors for testing\n",
    "large_a = torch.randn(1000, 1000)\n",
    "large_b = torch.randn(1000, 1000)\n",
    "\n",
    "# Out-of-place addition\n",
    "def out_of_place_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# In-place addition\n",
    "def in_place_add(a, b):\n",
    "    a_copy = a.clone()\n",
    "    a_copy.add_(b)\n",
    "    return a_copy\n",
    "\n",
    "# Time both operations\n",
    "out_of_place_time = time_operation(out_of_place_add, large_a, large_b)\n",
    "in_place_time = time_operation(in_place_add, large_a, large_b)\n",
    "\n",
    "print(f\"Out-of-place addition average time: {out_of_place_time:.6f} seconds\")\n",
    "print(f\"In-place addition average time: {in_place_time:.6f} seconds\")\n",
    "print(f\"In-place operations are {out_of_place_time/in_place_time:.2f}x faster\")\n",
    "\n",
    "# Memory contiguous example\n",
    "transposed = large_a.T\n",
    "print(f\"\\nTransposed tensor is contiguous: {transposed.is_contiguous()}\")\n",
    "\n",
    "contiguous = transposed.contiguous()\n",
    "print(f\"Contiguous version is contiguous: {contiguous.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375f2fd",
   "metadata": {},
   "source": [
    "## 9. Practical Applications in Deep Learning\n",
    "\n",
    "Let's see how tensors are used in practical deep learning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b05e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple neural network forward pass\n",
    "\n",
    "# Create sample data (batch_size=32, features=784 - like MNIST)\n",
    "batch_size, input_features, hidden_features, output_features = 32, 784, 128, 10\n",
    "x = torch.randn(batch_size, input_features)\n",
    "\n",
    "# Initialize weights and biases\n",
    "w1 = torch.randn(input_features, hidden_features)\n",
    "b1 = torch.randn(hidden_features)\n",
    "w2 = torch.randn(hidden_features, output_features)\n",
    "b2 = torch.randn(output_features)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"First layer weights shape: {w1.shape}\")\n",
    "print(f\"Second layer weights shape: {w2.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "h = torch.matmul(x, w1) + b1  # Linear transformation\n",
    "h = torch.relu(h)  # Activation function\n",
    "y = torch.matmul(h, w2) + b2  # Output layer\n",
    "\n",
    "print(f\"\\nHidden layer shape: {h.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# Softmax for probability distribution\n",
    "probabilities = torch.softmax(y, dim=1)\n",
    "print(f\"\\nProbabilities shape: {probabilities.shape}\")\n",
    "print(f\"Probabilities sum to 1: {torch.allclose(probabilities.sum(dim=1), torch.ones(batch_size))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd853ad",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "Here are some best practices for working with tensors in deep learning:\n",
    "\n",
    "1. **Use appropriate data types**: Choose the right dtype for your application (float32 for most cases, float16 for memory efficiency)\n",
    "2. **Leverage GPU acceleration**: Move tensors to GPU when available for faster computation\n",
    "3. **Use in-place operations**: When possible, use in-place operations to save memory\n",
    "4. **Ensure contiguous memory**: Use `.contiguous()` when needed for optimal performance\n",
    "5. **Batch operations**: Process data in batches to leverage vectorization\n",
    "6. **Memory management**: Be mindful of memory usage, especially with large tensors\n",
    "7. **Profiling**: Profile your code to identify performance bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa755e4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've covered the fundamentals of tensors in PyTorch:\n",
    "\n",
    "- What tensors are and how they generalize scalars, vectors, and matrices\n",
    "- How to create tensors with various initialization methods\n",
    "- Tensor properties including data types, shapes, and devices\n",
    "- Essential tensor operations from basic arithmetic to advanced linear algebra\n",
    "- Broadcasting rules for operations between tensors of different shapes\n",
    "- Indexing and slicing techniques for accessing tensor elements\n",
    "- Reshaping operations for manipulating tensor dimensions\n",
    "- Performance considerations including in-place operations and memory management\n",
    "- Practical applications in neural network forward passes\n",
    "\n",
    "Understanding these concepts is fundamental to working with deep learning frameworks and building complex models like the Large Language Models in this repository."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
