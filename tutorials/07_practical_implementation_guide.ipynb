{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "practical_implementation_header",
   "metadata": {},
   "source": [
    "# Practical Implementation Guide: From Theory to Code\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial serves as a bridge between the theoretical concepts covered in the lessons and the practical exercises. We'll walk through implementing a complete Transformer model from scratch, connecting each component to its theoretical foundation while providing practical coding examples.\n",
    "\n",
    "### What You'll Learn\n",
    "- How to translate theoretical concepts into working code\n",
    "- Implementation details of core Transformer components\n",
    "- Best practices for building modular and maintainable deep learning models\n",
    "- Techniques for debugging and testing neural network implementations\n",
    "- Performance optimization strategies for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical_implementation_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path('.').parent))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation_philosophy",
   "metadata": {},
   "source": [
    "## 1. Implementation Philosophy\n",
    "\n",
    "Before diving into code, let's establish some key principles for implementing deep learning models:\n",
    "\n",
    "1. **Modularity**: Break complex systems into smaller, reusable components\n",
    "2. **Clarity**: Prioritize readable code over clever optimizations\n",
    "3. **Testability**: Design components that can be easily tested in isolation\n",
    "4. **Extensibility**: Build systems that can be easily extended or modified\n",
    "5. **Performance**: Optimize bottlenecks without sacrificing clarity\n",
    "\n",
    "These principles will guide our implementation approach throughout this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention_implementation",
   "metadata": {},
   "source": [
    "## 2. Implementing Self-Attention\n",
    "\n",
    "Let's start by implementing the core attention mechanism. We'll connect this to the mathematical formulation from Lesson 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self_attention_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the scaled dot-product attention mechanism.\n",
    "    \n",
    "    Based on the formula: Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \n",
    "        Args:\n",
    "            query: Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
    "            key: Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
    "            value: Tensor of shape (batch_size, num_heads, seq_len, d_v)\n",
    "            mask: Optional mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (output, attention_weights)\n",
    "        \"\"\"\n",
    "        # Calculate attention scores: QK^T\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # Scale by square root of key dimension\n",
    "        d_k = query.size(-1)\n",
    "        scores = scores / math.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the attention implementation\n",
    "print(\"Testing ScaledDotProductAttention:\")\n",
    "\n",
    "# Create sample inputs\n",
    "batch_size, num_heads, seq_len, d_k, d_v = 2, 4, 8, 16, 16\n",
    "query = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "key = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "value = torch.randn(batch_size, num_heads, seq_len, d_v)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape: {value.shape}\")\n",
    "\n",
    "# Create attention module\n",
    "attention = ScaledDotProductAttention(dropout=0.1)\n",
    "\n",
    "# Compute attention\n",
    "start_time = time.time()\n",
    "output, weights = attention(query, key, value)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")\n",
    "\n",
    "# Verify attention weights sum to 1\n",
    "print(f\"Attention weights sum (should be ~1.0): {weights.sum(dim=-1)[0, 0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multihead_attention",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention Implementation\n",
    "\n",
    "Now let's implement multi-head attention, which runs several attention heads in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multihead_attention_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism that runs multiple attention heads in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.query_projection = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_projection = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_projection = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Compute multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            query: Tensor of shape (batch_size, seq_len, hidden_size)\n",
    "            key: Tensor of shape (batch_size, seq_len, hidden_size)\n",
    "            value: Tensor of shape (batch_size, seq_len, hidden_size)\n",
    "            mask: Optional mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (output, attention_weights)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.query_projection(query)\n",
    "        K = self.key_projection(key)\n",
    "        V = self.value_projection(value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        attention_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        # Shape: (batch_size, seq_len, hidden_size)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, -1, self.hidden_size)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.output_projection(attention_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "print(\"\\nTesting MultiHeadAttention:\")\n",
    "\n",
    "# Create sample inputs\n",
    "batch_size, seq_len, hidden_size, num_heads = 2, 8, 64, 8\n",
    "query = torch.randn(batch_size, seq_len, hidden_size)\n",
    "key = torch.randn(batch_size, seq_len, hidden_size)\n",
    "value = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "print(f\"Input shapes: query={query.shape}, key={key.shape}, value={value.shape}\")\n",
    "\n",
    "# Create multi-head attention module\n",
    "mha = MultiHeadAttention(hidden_size, num_heads, dropout=0.1)\n",
    "\n",
    "# Compute multi-head attention\n",
    "start_time = time.time()\n",
    "output, weights = mha(query, key, value)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positional_encoding",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding Implementation\n",
    "\n",
    "Let's implement positional encoding to provide sequence order information to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional_encoding_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding as described in 'Attention Is All You Need'.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute div_term for sine and cosine\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of the model state)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# Test positional encoding\n",
    "print(\"\\nTesting PositionalEncoding:\")\n",
    "\n",
    "# Create sample input\n",
    "seq_len, batch_size, d_model = 10, 2, 64\n",
    "x = torch.randn(seq_len, batch_size, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Create positional encoding module\n",
    "pos_encoding = PositionalEncoding(d_model, max_len=100)\n",
    "\n",
    "# Apply positional encoding\n",
    "start_time = time.time()\n",
    "output = pos_encoding(x)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")\n",
    "\n",
    "# Visualize positional encoding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pe_matrix = pos_encoding.pe[:seq_len, 0, :].detach().numpy()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pe_matrix, aspect='auto')\n",
    "plt.title('Positional Encoding Matrix')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed_forward_network",
   "metadata": {},
   "source": [
    "## 5. Feed-Forward Network Implementation\n",
    "\n",
    "Let's implement the position-wise feed-forward network used in Transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed_forward_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network used in Transformer layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply position-wise feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape\n",
    "        \"\"\"\n",
    "        # First linear transformation + activation\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)  # Using GELU activation as in many modern transformers\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second linear transformation\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test feed-forward network\n",
    "print(\"\\nTesting PositionwiseFeedForward:\")\n",
    "\n",
    "# Create sample input\n",
    "batch_size, seq_len, d_model, d_ff = 2, 8, 64, 256\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Create feed-forward network\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff, dropout=0.1)\n",
    "\n",
    "# Apply feed-forward network\n",
    "start_time = time.time()\n",
    "output = ffn(x)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layer_normalization",
   "metadata": {},
   "source": [
    "## 6. Layer Normalization Implementation\n",
    "\n",
    "Let's implement layer normalization, which is crucial for training deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "layer_norm_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization module.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply layer normalization.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, features)\n",
    "            \n",
    "        Returns:\n",
    "            Normalized tensor of same shape\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "# Test layer normalization\n",
    "print(\"\\nTesting LayerNorm:\")\n",
    "\n",
    "# Create sample input\n",
    "batch_size, seq_len, features = 2, 8, 64\n",
    "x = torch.randn(batch_size, seq_len, features)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Create layer normalization\n",
    "ln = LayerNorm(features)\n",
    "\n",
    "# Apply layer normalization\n",
    "start_time = time.time()\n",
    "output = ln(x)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")\n",
    "\n",
    "# Verify normalization properties\n",
    "print(f\"Output mean (should be ~0): {output.mean().item():.6f}\")\n",
    "print(f\"Output std (should be ~1): {output.std().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer_layer",
   "metadata": {},
   "source": [
    "## 7. Complete Transformer Layer Implementation\n",
    "\n",
    "Now let's combine all components into a complete Transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer_layer_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer layer consisting of multi-head attention and feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, ff_hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention sub-layer\n",
    "        self.attention = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.attention_norm = LayerNorm(hidden_size)\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed-forward sub-layer\n",
    "        self.feed_forward = PositionwiseFeedForward(hidden_size, ff_hidden_size, dropout)\n",
    "        self.ff_norm = LayerNorm(hidden_size)\n",
    "        self.ff_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Apply Transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, hidden_size)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection\n",
    "        attention_output, _ = self.attention(x, x, x, mask)\n",
    "        x = x + self.attention_dropout(attention_output)\n",
    "        x = self.attention_norm(x)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.ff_dropout(ff_output)\n",
    "        x = self.ff_norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test transformer layer\n",
    "print(\"\\nTesting TransformerLayer:\")\n",
    "\n",
    "# Create sample input\n",
    "batch_size, seq_len, hidden_size, num_heads, ff_hidden_size = 2, 8, 64, 8, 256\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Create transformer layer\n",
    "layer = TransformerLayer(hidden_size, num_heads, ff_hidden_size, dropout=0.1)\n",
    "\n",
    "# Apply transformer layer\n",
    "start_time = time.time()\n",
    "output = layer(x)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_transformer",
   "metadata": {},
   "source": [
    "## 8. Complete Transformer Model Implementation\n",
    "\n",
    "Finally, let's put everything together into a complete Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_transformer_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A complete Transformer model for sequence processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_heads, num_layers, ff_hidden_size, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = PositionalEncoding(hidden_size, max_seq_len)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_size, num_heads, ff_hidden_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.final_norm = LayerNorm(hidden_size)\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Process input sequence through the Transformer.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len) - token indices\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Logits tensor of shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Token embeddings\n",
    "        x = self.token_embedding(x) * math.sqrt(self.hidden_size)\n",
    "        \n",
    "        # Position embeddings (transpose for positional encoding)\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n",
    "        x = self.position_embedding(x)\n",
    "        x = x.transpose(0, 1)  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Apply dropout to embeddings\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test complete transformer model\n",
    "print(\"\\nTesting SimpleTransformer:\")\n",
    "\n",
    "# Model parameters\n",
    "vocab_size, hidden_size, num_heads, num_layers, ff_hidden_size = 1000, 64, 8, 4, 256\n",
    "batch_size, seq_len = 2, 16\n",
    "\n",
    "# Create sample input (token indices)\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "\n",
    "# Create transformer model\n",
    "model = SimpleTransformer(vocab_size, hidden_size, num_heads, num_layers, ff_hidden_size)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Apply transformer model\n",
    "start_time = time.time()\n",
    "logits = model(x)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Computation time: {elapsed_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_loop",
   "metadata": {},
   "source": [
    "## 9. Training Loop Implementation\n",
    "\n",
    "Let's implement a simple training loop to demonstrate how to train our Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(vocab_size, batch_size, seq_len, num_batches):\n",
    "    \"\"\"Create sample training data.\"\"\"\n",
    "    data = []\n",
    "    for _ in range(num_batches):\n",
    "        # Create random sequences\n",
    "        input_seq = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        target_seq = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        data.append((input_seq, target_seq))\n",
    "    return data\n",
    "\n",
    "def train_step(model, batch, optimizer, criterion, device):\n",
    "    \"\"\"Perform a single training step.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    input_seq, target_seq = batch\n",
    "    input_seq = input_seq.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_seq)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), target_seq.view(-1))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Create sample training data\n",
    "vocab_size, batch_size, seq_len, num_batches = 1000, 4, 32, 10\n",
    "train_data = create_sample_data(vocab_size, batch_size, seq_len, num_batches)\n",
    "\n",
    "print(\"\\nTraining Setup:\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Number of training batches: {num_batches}\")\n",
    "\n",
    "# Create model\n",
    "model = SimpleTransformer(vocab_size, hidden_size=128, num_heads=8, num_layers=4, ff_hidden_size=512)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):  # 3 epochs\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_data):\n",
    "        loss = train_step(model, batch, optimizer, criterion, device)\n",
    "        total_loss += loss\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"  Batch {batch_idx}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text_generation",
   "metadata": {},
   "source": [
    "## 10. Text Generation Implementation\n",
    "\n",
    "Let's implement a simple text generation function to demonstrate inference with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text_generation_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_ids, max_length=50, temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained transformer model\n",
    "        input_ids: Initial token sequence\n",
    "        max_length: Maximum length of generated sequence\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        device: Device to run generation on\n",
    "        \n",
    "    Returns:\n",
    "        Generated token sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with input sequence\n",
    "    generated = input_ids.clone().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            logits = model(generated)\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test text generation\n",
    "print(\"\\nTesting Text Generation:\")\n",
    "\n",
    "# Create sample input\n",
    "input_tokens = torch.randint(0, vocab_size, (1, 5)).to(device)  # 5 initial tokens\n",
    "print(f\"Input tokens: {input_tokens.cpu().numpy()[0]}\")\n",
    "\n",
    "# Generate text\n",
    "start_time = time.time()\n",
    "generated_tokens = generate_text(model, input_tokens, max_length=20, temperature=0.8, device=device)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"Generated tokens: {generated_tokens.cpu().numpy()[0]}\")\n",
    "print(f\"Input length: {input_tokens.size(1)}\")\n",
    "print(f\"Output length: {generated_tokens.size(1)}\")\n",
    "print(f\"Generation time: {generation_time*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_optimization",
   "metadata": {},
   "source": [
    "## 11. Performance Optimization Techniques\n",
    "\n",
    "Let's explore some performance optimization techniques for our Transformer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_optimization_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, input_shape, device, iterations=100):\n",
    "    \"\"\"Benchmark model performance.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create sample input\n",
    "    input_ids = torch.randint(0, 1000, input_shape).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_ids)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iterations):\n",
    "            _ = model(input_ids)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    avg_time = total_time / iterations\n",
    "    throughput = iterations / total_time\n",
    "    \n",
    "    return avg_time, throughput\n",
    "\n",
    "# Benchmark our model\n",
    "print(\"\\nPerformance Benchmarking:\")\n",
    "\n",
    "# Test with different batch sizes\n",
    "test_batch_sizes = [1, 2, 4, 8]\n",
    "seq_len = 32\n",
    "\n",
    "for batch_size in test_batch_sizes:\n",
    "    input_shape = (batch_size, seq_len)\n",
    "    avg_time, throughput = benchmark_model(model, input_shape, device, iterations=50)\n",
    "    print(f\"Batch size {batch_size:2d}: {avg_time*1000:6.2f} ms/batch, {throughput:6.2f} batches/sec\")\n",
    "\n",
    "# Test with different sequence lengths\n",
    "batch_size = 4\n",
    "test_seq_lengths = [16, 32, 64, 128]\n",
    "\n",
    "print(\"\\nSequence Length Performance:\")\n",
    "for seq_len in test_seq_lengths:\n",
    "    input_shape = (batch_size, seq_len)\n",
    "    avg_time, throughput = benchmark_model(model, input_shape, device, iterations=50)\n",
    "    print(f\"Seq length {seq_len:3d}: {avg_time*1000:6.2f} ms/batch, {throughput:6.2f} batches/sec\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation_best_practices",
   "metadata": {},
   "source": [
    "## 12. Implementation Best Practices\n",
    "\n",
    "Let's summarize the key best practices we've demonstrated in this implementation:\n",
    "\n",
    "### 1. Modular Design\n",
    "- Each component (attention, feed-forward, etc.) is implemented as a separate module\n",
    "- Easy to test, debug, and reuse components\n",
    "\n",
    "### 2. Clear Documentation\n",
    "- Comprehensive docstrings for all classes and methods\n",
    "- Clear variable names and comments\n",
    "\n",
    "### 3. Error Handling\n",
    "- Assertions to catch configuration errors\n",
    "- Proper tensor shape handling\n",
    "\n",
    "### 4. Performance Considerations\n",
    "- Efficient tensor operations\n",
    "- Proper use of in-place operations\n",
    "- Memory-efficient implementation\n",
    "\n",
    "### 5. Testing and Validation\n",
    "- Each component tested independently\n",
    "- Shape verification\n",
    "- Numerical validation (e.g., attention weights sum to 1)\n",
    "\n",
    "### 6. Extensibility\n",
    "- Easy to modify components\n",
    "- Configurable hyperparameters\n",
    "- Clear interfaces between components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connecting_to_exercises",
   "metadata": {},
   "source": [
    "## 13. Connecting to Exercises\n",
    "\n",
    "This implementation directly connects to the exercises in several ways:\n",
    "\n",
    "1. **Exercise 3 (Simple Transformer)**: Our implementation provides a complete solution that you can use as a reference\n",
    "2. **Exercise 2 (Tokenizer)**: The token embedding layer works with the tokenizer you implemented\n",
    "3. **Exercise 1 (Tensor Operations)**: All operations use the tensor concepts you learned\n",
    "\n",
    "### How to Use This as a Foundation\n",
    "\n",
    "1. **Start Simple**: Begin with the basic components (attention, feed-forward)\n",
    "2. **Test Incrementally**: Validate each component before combining\n",
    "3. **Profile Performance**: Use the benchmarking code to identify bottlenecks\n",
    "4. **Extend Gradually**: Add features like masking, different activation functions, etc.\n",
    "\n",
    "### Challenge Extensions\n",
    "\n",
    "To further develop your skills, consider these extensions:\n",
    "\n",
    "1. **Add Caching**: Implement attention caching for faster autoregressive generation\n",
    "2. **Mixed Precision**: Use torch.cuda.amp for memory-efficient training\n",
    "3. **Distributed Training**: Implement data parallelism across multiple GPUs\n",
    "4. **Advanced Attention**: Try relative positional encoding or sparse attention\n",
    "5. **Model Compression**: Implement pruning or quantization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've bridged the gap between theoretical concepts and practical implementation:\n",
    "\n",
    "- **Connected Theory to Practice**: We've shown how mathematical formulations translate to working code\n",
    "- **Implemented Core Components**: Built each Transformer component from scratch with clear explanations\n",
    "- **Demonstrated Best Practices**: Showed modular design, testing, and optimization techniques\n",
    "- **Provided a Complete Example**: Created a full Transformer model with training and inference\n",
    "- **Connected to Exercises**: Linked the implementation to the practical exercises in the curriculum\n",
    "\n",
    "This implementation serves as a solid foundation that you can extend and modify for your own projects. The modular design makes it easy to experiment with different components, and the comprehensive testing ensures reliability.\n",
    "\n",
    "Remember that real-world implementations often include additional optimizations and features, but this tutorial provides the essential foundation for understanding how Transformers work under the hood."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
